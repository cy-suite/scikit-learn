# -*- coding: utf-8 -*-
"""Untitled.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/171EX1Jmtcs0SFC8RaCiaya4bDH_tuPY1
"""

import pickle
import pandas as pd
from scipy.sparse import csr_matrix
from sklearn.linear_model import SGDClassifier

# Load the dataset from a pickle file
with open("linearSVC_data_dict.pkl", "rb") as f:
    data = pickle.load(f)

# Display the loaded data
for key, value in data.items():
    print(f"{key}:")
    print(value)
    print()

"""# Data types optimization"""

# Convert the training data to a CSR (Compressed Sparse Row) matrix with int16 data type if it's not already in that format
if not isinstance(data["train"], csr_matrix):
    X_train_sparse = csr_matrix(data["train"], dtype='int16')
else:
    X_train_sparse = data["train"].astype('int16')

# Create a DataFrame from the sparse matrix
df = pd.DataFrame.sparse.from_spmatrix(X_train_sparse)

# Add the target column to the DataFrame and change its data type to int16
df['target'] = data["target"].astype('int16')

# Display the DataFrame information
print(df.info())

# Assign the target values to y_train
y_train = data["target"]

# Train the SGDClassifier with hinge loss (approximation of SVM)
# Use all available CPU cores for parallelization (n_jobs=-1)
# Enable early stopping to terminate training when the model's performance on the validation set stops improving
# Set the size of the validation set to 10% of the training data (validation_fraction=0.1)
model = SGDClassifier(loss='hinge', random_state=42, n_jobs=-1, early_stopping=True, validation_fraction=0.1)
model.fit(X_train_sparse, y_train)

print("Model trained successfully.")

from sklearn.linear_model import SGDClassifier
import numpy as np

model = SGDClassifier(loss='hinge', max_iter=1000, tol=1e-3,n_jobs=-1)

# Process data in chunks
chunk_size = 1000  # Adjust based on memory constraints
n_samples = X_train_sparse.shape[0]
for i in range(0, n_samples, chunk_size):
    X_chunk = X_train_sparse[i:i + chunk_size]
    y_chunk = y_train[i:i + chunk_size]
    model.partial_fit(X_chunk, y_chunk, classes=np.unique(y_train))

print("Model trained successfully.")





