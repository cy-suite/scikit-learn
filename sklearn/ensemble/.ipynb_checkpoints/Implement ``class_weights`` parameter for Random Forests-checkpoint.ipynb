{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division\n",
      "\n",
      "import itertools\n",
      "import numpy as np\n",
      "from warnings import warn\n",
      "from abc import ABCMeta, abstractmethod\n",
      "\n",
      "from sklearn.base import ClassifierMixin, RegressorMixin\n",
      "from sklearn.externals.joblib import Parallel, delayed\n",
      "from sklearn.externals import six\n",
      "from sklearn.externals.six.moves import xrange\n",
      "from sklearn.feature_selection.from_model import _LearntSelectorMixin\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "from sklearn.tree import (DecisionTreeClassifier, DecisionTreeRegressor,\n",
      "                    ExtraTreeClassifier, ExtraTreeRegressor)\n",
      "from sklearn.tree._tree import DTYPE, DOUBLE\n",
      "from sklearn.utils import array2d, check_random_state, check_arrays, safe_asarray\n",
      "from sklearn.utils.validation import DataConversionWarning\n",
      "from sklearn.utils.fixes import bincount, unique\n",
      "from sklearn.utils.class_weight import compute_class_weight\n",
      "\n",
      "from sklearn.ensemble.base import BaseEnsemble"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "__all__ = [\"RandomForestClassifier\",\n",
      "           \"RandomForestRegressor\",\n",
      "           \"ExtraTreesClassifier\",\n",
      "           \"ExtraTreesRegressor\"]\n",
      "\n",
      "MAX_INT = np.iinfo(np.int32).max"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def _parallel_build_trees(trees, forest, X, y, classes, class_weight,  sample_weight, verbose):\n",
      "    \"\"\"Private function used to build a batch of trees within a job.\"\"\"\n",
      "    for i, tree in enumerate(trees):\n",
      "        if verbose > 1:\n",
      "            print(\"building tree %d of %d\" % (i + 1, len(trees)))\n",
      "\n",
      "        if forest.bootstrap:\n",
      "            n_samples = X.shape[0]\n",
      "            if sample_weight is None:\n",
      "                curr_sample_weight = np.ones((n_samples,), dtype=np.float64)\n",
      "            else:\n",
      "                curr_sample_weight = sample_weight.copy()\n",
      "\n",
      "            random_state = check_random_state(tree.random_state)\n",
      "            indices = random_state.randint(0, n_samples, n_samples)\n",
      "            sample_counts = bincount(indices, minlength=n_samples)\n",
      "            curr_sample_weight *= sample_counts\n",
      "\n",
      "            tree.fit(X, y,\n",
      "                     sample_weight=curr_sample_weight,\n",
      "                     check_input=False)\n",
      "\n",
      "            tree.indices_ = sample_counts > 0.\n",
      "\n",
      "        else:\n",
      "            tree.fit(X, y,\n",
      "                     sample_weight=sample_weight,\n",
      "                     check_input=False)\n",
      "\n",
      "    return trees\n",
      "\n",
      "\n",
      "def _parallel_predict_proba(trees, X, n_classes, n_outputs):\n",
      "    \"\"\"Private function used to compute a batch of predictions within a job.\"\"\"\n",
      "    n_samples = X.shape[0]\n",
      "\n",
      "    if n_outputs == 1:\n",
      "        proba = np.zeros((n_samples, n_classes))\n",
      "\n",
      "        for tree in trees:\n",
      "            proba_tree = tree.predict_proba(X)\n",
      "\n",
      "            if n_classes == tree.n_classes_:\n",
      "                proba += proba_tree\n",
      "\n",
      "            else:\n",
      "                proba[:, tree.classes_] += \\\n",
      "                    proba_tree[:, range(len(tree.classes_))]\n",
      "\n",
      "    else:\n",
      "        proba = []\n",
      "\n",
      "        for k in xrange(n_outputs):\n",
      "            proba.append(np.zeros((n_samples, n_classes[k])))\n",
      "\n",
      "        for tree in trees:\n",
      "            proba_tree = tree.predict_proba(X)\n",
      "\n",
      "            for k in xrange(n_outputs):\n",
      "                if n_classes[k] == tree.n_classes_[k]:\n",
      "                    proba[k] += proba_tree[k]\n",
      "\n",
      "                else:\n",
      "                    proba[k][:, tree.classes_] += \\\n",
      "                        proba_tree[k][:, range(len(tree.classes_))]\n",
      "\n",
      "    return proba\n",
      "\n",
      "\n",
      "def _parallel_predict_regression(trees, X):\n",
      "    \"\"\"Private function used to compute a batch of predictions within a job.\"\"\"\n",
      "    return sum(tree.predict(X) for tree in trees)\n",
      "\n",
      "\n",
      "def _parallel_apply(tree, X):\n",
      "    \"\"\"Private helper function for parallizing calls to apply in a forest.\"\"\"\n",
      "    return tree.tree_.apply(X)\n",
      "\n",
      "\n",
      "class BaseForest(six.with_metaclass(ABCMeta, BaseEnsemble,\n",
      "                                    _LearntSelectorMixin)):\n",
      "    \"\"\"Base class for forests of trees.\n",
      "\n",
      "    Warning: This class should not be used directly. Use derived classes\n",
      "    instead.\n",
      "    \"\"\"\n",
      "\n",
      "    @abstractmethod\n",
      "    def __init__(self,\n",
      "                 base_estimator,\n",
      "                 n_estimators=10,\n",
      "                 estimator_params=tuple(),\n",
      "                 bootstrap=False,\n",
      "                 oob_score=False,\n",
      "                 n_jobs=1,\n",
      "                 random_state=None,\n",
      "                 verbose=0):\n",
      "        super(BaseForest, self).__init__(\n",
      "            base_estimator=base_estimator,\n",
      "            n_estimators=n_estimators,\n",
      "            estimator_params=estimator_params)\n",
      "\n",
      "        self.bootstrap = bootstrap\n",
      "        self.oob_score = oob_score\n",
      "        self.n_jobs = n_jobs\n",
      "        self.random_state = random_state\n",
      "        self.verbose = verbose\n",
      "\n",
      "    def apply(self, X):\n",
      "        \"\"\"Apply trees in the forest to X, return leaf indices.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like, shape = [n_samples, n_features]\n",
      "            Input data.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        X_leaves : array_like, shape = [n_samples, n_estimators]\n",
      "            For each datapoint x in X and for each tree in the forest,\n",
      "            return the index of the leaf x ends up in.\n",
      "        \"\"\"\n",
      "        X = array2d(X, dtype=DTYPE)\n",
      "        results = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "                           backend=\"threading\")(\n",
      "            delayed(_parallel_apply)(tree, X) for tree in self.estimators_)\n",
      "        return np.array(results).T\n",
      "\n",
      "    def fit(self, X, y, sample_weight=None):\n",
      "        \"\"\"Build a forest of trees from the training set (X, y).\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape = [n_samples, n_features]\n",
      "            The training input samples.\n",
      "\n",
      "        y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
      "            The target values (integers that correspond to classes in\n",
      "            classification, real numbers in regression).\n",
      "\n",
      "        sample_weight : array-like, shape = [n_samples] or None\n",
      "            Sample weights. If None, then samples are equally weighted. Splits\n",
      "            that would create child nodes with net zero or negative weight are\n",
      "            ignored while searching for a split in each node. In the case of\n",
      "            classification, splits are also ignored if they would result in any\n",
      "            single class carrying a negative weight in either child node.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        self : object\n",
      "            Returns self.\n",
      "        \"\"\"\n",
      "        random_state = check_random_state(self.random_state)\n",
      "\n",
      "        # Convert data\n",
      "        X, = check_arrays(X, dtype=DTYPE, sparse_format=\"dense\")\n",
      "\n",
      "        # Remap output\n",
      "        n_samples, self.n_features_ = X.shape\n",
      "\n",
      "        y = np.atleast_1d(y)\n",
      "        if y.ndim == 2 and y.shape[1] == 1:\n",
      "            warn(\"A column-vector y was passed when a 1d array was\"\n",
      "                 \" expected. Please change the shape of y to \"\n",
      "                 \"(n_samples, ), for example using ravel().\",\n",
      "                 DataConversionWarning, stacklevel=2)\n",
      "\n",
      "        if y.ndim == 1:\n",
      "            # reshape is necessary to preserve the data contiguity against vs\n",
      "            # [:, np.newaxis] that does not.\n",
      "            y = np.reshape(y, (-1, 1))\n",
      "\n",
      "        self.n_outputs_ = y.shape[1]\n",
      "\n",
      "        y = self._validate_y(y)\n",
      "\n",
      "        if getattr(y, \"dtype\", None) != DOUBLE or not y.flags.contiguous:\n",
      "            y = np.ascontiguousarray(y, dtype=DOUBLE)\n",
      "\n",
      "        # Check parameters\n",
      "        self._validate_estimator()\n",
      "\n",
      "        if not self.bootstrap and self.oob_score:\n",
      "            raise ValueError(\"Out of bag estimation only available\"\n",
      "                             \" if bootstrap=True\")\n",
      "\n",
      "        # Assign chunk of trees to jobs\n",
      "        n_jobs, n_trees, starts = _partition_estimators(self)\n",
      "        trees = []\n",
      "\n",
      "        for i in range(self.n_estimators):\n",
      "            tree = self._make_estimator(append=False)\n",
      "            tree.set_params(random_state=random_state.randint(MAX_INT))\n",
      "            trees.append(tree)\n",
      "\n",
      "        # Free allocated memory, if any\n",
      "        self.estimators_ = None\n",
      "\n",
      "        # Parallel loop: we use the threading backend as the Cython code for\n",
      "        # fitting the trees is internally releasing the Python GIL making\n",
      "        # threading always more efficient than multiprocessing in that case.\n",
      "        all_trees = Parallel(n_jobs=n_jobs, verbose=self.verbose,\n",
      "                             backend=\"threading\")(\n",
      "            delayed(_parallel_build_trees)(\n",
      "                trees[starts[i]:starts[i + 1]],\n",
      "                self,\n",
      "                X,\n",
      "                y,\n",
      "                sample_weight,\n",
      "                verbose=self.verbose)\n",
      "            for i in range(n_jobs))\n",
      "\n",
      "        # Reduce\n",
      "        self.estimators_ = list(itertools.chain(*all_trees))\n",
      "\n",
      "        if self.oob_score:\n",
      "            self._set_oob_score(X, y)\n",
      "\n",
      "        # Decapsulate classes_ attributes\n",
      "        if hasattr(self, \"classes_\") and self.n_outputs_ == 1:\n",
      "            self.n_classes_ = self.n_classes_[0]\n",
      "            self.classes_ = self.classes_[0]\n",
      "\n",
      "        return self\n",
      "\n",
      "    @abstractmethod\n",
      "    def _set_oob_score(self, X, y):\n",
      "        \"\"\"Calculate out of bag predictions and score.\"\"\"\n",
      "\n",
      "    def _validate_y(self, y):\n",
      "        # Default implementation\n",
      "        return y\n",
      "\n",
      "    @property\n",
      "    def feature_importances_(self):\n",
      "        \"\"\"Return the feature importances (the higher, the more important the\n",
      "           feature).\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        feature_importances_ : array, shape = [n_features]\n",
      "        \"\"\"\n",
      "        if self.estimators_ is None or len(self.estimators_) == 0:\n",
      "            raise ValueError(\"Estimator not fitted, \"\n",
      "                             \"call `fit` before `feature_importances_`.\")\n",
      "\n",
      "        return sum(tree.feature_importances_\n",
      "                   for tree in self.estimators_) / self.n_estimators\n",
      "\n",
      "\n",
      "class ForestClassifier(six.with_metaclass(ABCMeta, BaseForest,\n",
      "                                          ClassifierMixin)):\n",
      "    \"\"\"Base class for forest of trees-based classifiers.\n",
      "\n",
      "    Warning: This class should not be used directly. Use derived classes\n",
      "    instead.\n",
      "    \"\"\"\n",
      "\n",
      "    @abstractmethod\n",
      "    def __init__(self,\n",
      "                 base_estimator,\n",
      "                 n_estimators=10,\n",
      "                 estimator_params=tuple(),\n",
      "                 bootstrap=False,\n",
      "                 oob_score=False,\n",
      "                 n_jobs=1,\n",
      "                 random_state=None,\n",
      "                 verbose=0):\n",
      "\n",
      "        super(ForestClassifier, self).__init__(\n",
      "            base_estimator,\n",
      "            n_estimators=n_estimators,\n",
      "            estimator_params=estimator_params,\n",
      "            bootstrap=bootstrap,\n",
      "            oob_score=oob_score,\n",
      "            n_jobs=n_jobs,\n",
      "            random_state=random_state,\n",
      "            verbose=verbose)\n",
      "\n",
      "    def _set_oob_score(self, X, y):\n",
      "        n_classes_ = self.n_classes_\n",
      "        n_samples = y.shape[0]\n",
      "\n",
      "        oob_decision_function = []\n",
      "        oob_score = 0.0\n",
      "        predictions = []\n",
      "\n",
      "        for k in xrange(self.n_outputs_):\n",
      "            predictions.append(np.zeros((n_samples,\n",
      "                                         n_classes_[k])))\n",
      "\n",
      "        for estimator in self.estimators_:\n",
      "            mask = np.ones(n_samples, dtype=np.bool)\n",
      "            mask[estimator.indices_] = False\n",
      "            p_estimator = estimator.predict_proba(X[mask, :])\n",
      "\n",
      "            if self.n_outputs_ == 1:\n",
      "                p_estimator = [p_estimator]\n",
      "\n",
      "            for k in xrange(self.n_outputs_):\n",
      "                predictions[k][mask, :] += p_estimator[k]\n",
      "\n",
      "        for k in xrange(self.n_outputs_):\n",
      "            if (predictions[k].sum(axis=1) == 0).any():\n",
      "                warn(\"Some inputs do not have OOB scores. \"\n",
      "                     \"This probably means too few trees were used \"\n",
      "                     \"to compute any reliable oob estimates.\")\n",
      "\n",
      "            decision = (predictions[k] /\n",
      "                        predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "            oob_decision_function.append(decision)\n",
      "            oob_score += np.mean(y[:, k] ==\n",
      "                                 np.argmax(predictions[k], axis=1), axis=0)\n",
      "\n",
      "        if self.n_outputs_ == 1:\n",
      "            self.oob_decision_function_ = oob_decision_function[0]\n",
      "        else:\n",
      "            self.oob_decision_function_ = oob_decision_function\n",
      "\n",
      "        self.oob_score_ = oob_score / self.n_outputs_\n",
      "\n",
      "    def _validate_y(self, y):\n",
      "        y = np.copy(y)\n",
      "\n",
      "        self.classes_ = []\n",
      "        self.n_classes_ = []\n",
      "\n",
      "        for k in xrange(self.n_outputs_):\n",
      "            classes_k, y[:, k] = unique(y[:, k], return_inverse=True)\n",
      "            self.classes_.append(classes_k)\n",
      "            self.n_classes_.append(classes_k.shape[0])\n",
      "\n",
      "        return y\n",
      "\n",
      "    def predict(self, X):\n",
      "        \"\"\"Predict class for X.\n",
      "\n",
      "        The predicted class of an input sample is computed as the majority\n",
      "        prediction of the trees in the forest.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape = [n_samples, n_features]\n",
      "            The input samples.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        y : array of shape = [n_samples] or [n_samples, n_outputs]\n",
      "            The predicted classes.\n",
      "        \"\"\"\n",
      "        n_samples = len(X)\n",
      "        proba = self.predict_proba(X)\n",
      "\n",
      "        if self.n_outputs_ == 1:\n",
      "            return self.classes_.take(np.argmax(proba, axis=1), axis=0)\n",
      "\n",
      "        else:\n",
      "            predictions = np.zeros((n_samples, self.n_outputs_))\n",
      "\n",
      "            for k in xrange(self.n_outputs_):\n",
      "                predictions[:, k] = self.classes_[k].take(np.argmax(proba[k],\n",
      "                                                                    axis=1),\n",
      "                                                          axis=0)\n",
      "\n",
      "            return predictions\n",
      "\n",
      "    def predict_proba(self, X):\n",
      "        \"\"\"Predict class probabilities for X.\n",
      "\n",
      "        The predicted class probabilities of an input sample is computed as\n",
      "        the mean predicted class probabilities of the trees in the forest.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape = [n_samples, n_features]\n",
      "            The input samples.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        p : array of shape = [n_samples, n_classes], or a list of n_outputs\n",
      "            such arrays if n_outputs > 1.\n",
      "            The class probabilities of the input samples. Classes are\n",
      "            ordered by arithmetical order.\n",
      "        \"\"\"\n",
      "        # Check data\n",
      "        if getattr(X, \"dtype\", None) != DTYPE or X.ndim != 2:\n",
      "            X = array2d(X, dtype=DTYPE)\n",
      "\n",
      "        # Assign chunk of trees to jobs\n",
      "        n_jobs, n_trees, starts = _partition_estimators(self)\n",
      "\n",
      "        # Parallel loop\n",
      "        all_proba = Parallel(n_jobs=n_jobs, verbose=self.verbose,\n",
      "                             backend=\"threading\")(\n",
      "            delayed(_parallel_predict_proba)(\n",
      "                self.estimators_[starts[i]:starts[i + 1]],\n",
      "                X,\n",
      "                self.n_classes_,\n",
      "                self.n_outputs_)\n",
      "            for i in range(n_jobs))\n",
      "\n",
      "        # Reduce\n",
      "        proba = all_proba[0]\n",
      "\n",
      "        if self.n_outputs_ == 1:\n",
      "            for j in xrange(1, len(all_proba)):\n",
      "                proba += all_proba[j]\n",
      "\n",
      "            proba /= len(self.estimators_)\n",
      "\n",
      "        else:\n",
      "            for j in xrange(1, len(all_proba)):\n",
      "                for k in xrange(self.n_outputs_):\n",
      "                    proba[k] += all_proba[j][k]\n",
      "\n",
      "            for k in xrange(self.n_outputs_):\n",
      "                proba[k] /= self.n_estimators\n",
      "\n",
      "        return proba\n",
      "\n",
      "    def predict_log_proba(self, X):\n",
      "        \"\"\"Predict class log-probabilities for X.\n",
      "\n",
      "        The predicted class log-probabilities of an input sample is computed as\n",
      "        the log of the mean predicted class probabilities of the trees in the\n",
      "        forest.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape = [n_samples, n_features]\n",
      "            The input samples.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        p : array of shape = [n_samples, n_classes], or a list of n_outputs\n",
      "            such arrays if n_outputs > 1.\n",
      "            The class log-probabilities of the input samples. Classes are\n",
      "            ordered by arithmetical order.\n",
      "        \"\"\"\n",
      "        proba = self.predict_proba(X)\n",
      "\n",
      "        if self.n_outputs_ == 1:\n",
      "            return np.log(proba)\n",
      "\n",
      "        else:\n",
      "            for k in xrange(self.n_outputs_):\n",
      "                proba[k] = np.log(proba[k])\n",
      "\n",
      "            return proba\n",
      "\n",
      "\n",
      "class ForestRegressor(six.with_metaclass(ABCMeta, BaseForest, RegressorMixin)):\n",
      "    \"\"\"Base class for forest of trees-based regressors.\n",
      "\n",
      "    Warning: This class should not be used directly. Use derived classes\n",
      "    instead.\n",
      "    \"\"\"\n",
      "\n",
      "    @abstractmethod\n",
      "    def __init__(self,\n",
      "                 base_estimator,\n",
      "                 n_estimators=10,\n",
      "                 estimator_params=tuple(),\n",
      "                 bootstrap=False,\n",
      "                 oob_score=False,\n",
      "                 n_jobs=1,\n",
      "                 random_state=None,\n",
      "                 verbose=0):\n",
      "        super(ForestRegressor, self).__init__(\n",
      "            base_estimator,\n",
      "            n_estimators=n_estimators,\n",
      "            estimator_params=estimator_params,\n",
      "            bootstrap=bootstrap,\n",
      "            oob_score=oob_score,\n",
      "            n_jobs=n_jobs,\n",
      "            random_state=random_state,\n",
      "            verbose=verbose)\n",
      "\n",
      "    def predict(self, X):\n",
      "        \"\"\"Predict regression target for X.\n",
      "\n",
      "        The predicted regression target of an input sample is computed as the\n",
      "        mean predicted regression targets of the trees in the forest.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape = [n_samples, n_features]\n",
      "            The input samples.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        y: array of shape = [n_samples] or [n_samples, n_outputs]\n",
      "            The predicted values.\n",
      "        \"\"\"\n",
      "        # Check data\n",
      "        if getattr(X, \"dtype\", None) != DTYPE or X.ndim != 2:\n",
      "            X = array2d(X, dtype=DTYPE)\n",
      "\n",
      "        # Assign chunk of trees to jobs\n",
      "        n_jobs, n_trees, starts = _partition_estimators(self)\n",
      "\n",
      "        # Parallel loop\n",
      "        all_y_hat = Parallel(n_jobs=n_jobs, verbose=self.verbose,\n",
      "                             backend=\"threading\")(\n",
      "            delayed(_parallel_predict_regression)(\n",
      "                self.estimators_[starts[i]:starts[i + 1]], X)\n",
      "            for i in range(n_jobs))\n",
      "\n",
      "        # Reduce\n",
      "        y_hat = sum(all_y_hat) / len(self.estimators_)\n",
      "\n",
      "        return y_hat\n",
      "\n",
      "    def _set_oob_score(self, X, y):\n",
      "        n_samples = y.shape[0]\n",
      "\n",
      "        predictions = np.zeros((n_samples, self.n_outputs_))\n",
      "        n_predictions = np.zeros((n_samples, self.n_outputs_))\n",
      "\n",
      "        for estimator in self.estimators_:\n",
      "            mask = np.ones(n_samples, dtype=np.bool)\n",
      "            mask[estimator.indices_] = False\n",
      "            p_estimator = estimator.predict(X[mask, :])\n",
      "\n",
      "            if self.n_outputs_ == 1:\n",
      "                p_estimator = p_estimator[:, np.newaxis]\n",
      "\n",
      "            predictions[mask, :] += p_estimator\n",
      "            n_predictions[mask, :] += 1\n",
      "\n",
      "        if (n_predictions == 0).any():\n",
      "            warn(\"Some inputs do not have OOB scores. \"\n",
      "                 \"This probably means too few trees were used \"\n",
      "                 \"to compute any reliable oob estimates.\")\n",
      "            n_predictions[n_predictions == 0] = 1\n",
      "\n",
      "        predictions /= n_predictions\n",
      "        self.oob_prediction_ = predictions\n",
      "\n",
      "        if self.n_outputs_ == 1:\n",
      "            self.oob_prediction_ = \\\n",
      "                self.oob_prediction_.reshape((n_samples, ))\n",
      "\n",
      "        self.oob_score_ = 0.0\n",
      "\n",
      "        for k in xrange(self.n_outputs_):\n",
      "            self.oob_score_ += r2_score(y[:, k],\n",
      "                                        predictions[:, k])\n",
      "\n",
      "        self.oob_score_ /= self.n_outputs_\n",
      "\n",
      "\n",
      "class RandomForestClassifier(ForestClassifier):\n",
      "    \"\"\"A random forest classifier.\n",
      "\n",
      "    A random forest is a meta estimator that fits a number of decision tree\n",
      "    classifiers on various sub-samples of the dataset and use averaging to\n",
      "    improve the predictive accuracy and control over-fitting.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    n_estimators : integer, optional (default=10)\n",
      "        The number of trees in the forest.\n",
      "\n",
      "    criterion : string, optional (default=\"gini\")\n",
      "        The function to measure the quality of a split. Supported criteria are\n",
      "        \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
      "        Note: this parameter is tree-specific.\n",
      "\n",
      "    max_features : int, float, string or None, optional (default=\"auto\")\n",
      "        The number of features to consider when looking for the best split:\n",
      "          - If int, then consider `max_features` features at each split.\n",
      "          - If float, then `max_features` is a percentage and\n",
      "            `int(max_features * n_features)` features are considered at each\n",
      "            split.\n",
      "          - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      "          - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "          - If \"log2\", then `max_features=log2(n_features)`.\n",
      "          - If None, then `max_features=n_features`.\n",
      "\n",
      "        Note: this parameter is tree-specific.\n",
      "\n",
      "    max_depth : integer or None, optional (default=None)\n",
      "        The maximum depth of the tree. If None, then nodes are expanded until\n",
      "        all leaves are pure or until all leaves contain less than\n",
      "        min_samples_split samples.\n",
      "        Ignored if ``max_samples_leaf`` is not None.\n",
      "        Note: this parameter is tree-specific.\n",
      "\n",
      "    min_samples_split : integer, optional (default=2)\n",
      "        The minimum number of samples required to split an internal node.\n",
      "        Note: this parameter is tree-specific.\n",
      "\n",
      "    min_samples_leaf : integer, optional (default=1)\n",
      "        The minimum number of samples in newly created leaves.  A split is\n",
      "        discarded if after the split, one of the leaves would contain less then\n",
      "        ``min_samples_leaf`` samples.\n",
      "        Note: this parameter is tree-specific.\n",
      "\n",
      "    max_leaf_nodes : int or None, optional (default=None)\n",
      "        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "        Best nodes are defined as relative reduction in impurity.\n",
      "        If None then unlimited number of leaf nodes.\n",
      "        If not None then ``max_depth`` will be ignored.\n",
      "        Note: this parameter is tree-specific.\n",
      "\n",
      "    bootstrap : boolean, optional (default=True)\n",
      "        Whether bootstrap samples are used when building trees.\n",
      "\n",
      "    oob_score : bool\n",
      "        Whether to use out-of-bag samples to estimate\n",
      "        the generalization error.\n",
      "\n",
      "    n_jobs : integer, optional (default=1)\n",
      "        The number of jobs to run in parallel for both `fit` and `predict`.\n",
      "        If -1, then the number of jobs is set to the number of cores.\n",
      "\n",
      "    random_state : int, RandomState instance or None, optional (default=None)\n",
      "        If int, random_state is the seed used by the random number generator;\n",
      "        If RandomState instance, random_state is the random number generator;\n",
      "        If None, the random number generator is the RandomState instance used\n",
      "        by `np.random`.\n",
      "\n",
      "    verbose : int, optional (default=0)\n",
      "        Controls the verbosity of the tree building process.\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    `estimators_`: list of DecisionTreeClassifier\n",
      "        The collection of fitted sub-estimators.\n",
      "\n",
      "    `classes_`: array of shape = [n_classes] or a list of such arrays\n",
      "        The classes labels (single output problem), or a list of arrays of\n",
      "        class labels (multi-output problem).\n",
      "\n",
      "    `n_classes_`: int or list\n",
      "        The number of classes (single output problem), or a list containing the\n",
      "        number of classes for each output (multi-output problem).\n",
      "\n",
      "    `feature_importances_` : array of shape = [n_features]\n",
      "        The feature importances (the higher, the more important the feature).\n",
      "\n",
      "    `oob_score_` : float\n",
      "        Score of the training dataset obtained using an out-of-bag estimate.\n",
      "\n",
      "    `oob_decision_function_` : array of shape = [n_samples, n_classes]\n",
      "        Decision function computed with out-of-bag estimate on the training\n",
      "        set. If n_estimators is small it might be possible that a data point\n",
      "        was never left out during the bootstrap. In this case,\n",
      "        `oob_decision_function_` might contain NaN.\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "\n",
      "    .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      "\n",
      "    See also\n",
      "    --------\n",
      "    DecisionTreeClassifier, ExtraTreesClassifier\n",
      "    \"\"\"\n",
      "    def __init__(self,\n",
      "                 n_estimators=10,\n",
      "                 criterion=\"gini\",\n",
      "                 max_depth=None,\n",
      "                 min_samples_split=2,\n",
      "                 min_samples_leaf=1,\n",
      "                 max_features=\"auto\",\n",
      "                 max_leaf_nodes=None,\n",
      "                 bootstrap=True,\n",
      "                 oob_score=False,\n",
      "                 n_jobs=1,\n",
      "                 random_state=None,\n",
      "                 verbose=0,\n",
      "                 min_density=None,\n",
      "                 compute_importances=None):\n",
      "        super(RandomForestClassifier, self).__init__(\n",
      "            base_estimator=DecisionTreeClassifier(),\n",
      "            n_estimators=n_estimators,\n",
      "            estimator_params=(\"criterion\", \"max_depth\", \"min_samples_split\",\n",
      "                              \"min_samples_leaf\", \"max_features\",\n",
      "                              \"max_leaf_nodes\", \"random_state\"),\n",
      "            bootstrap=bootstrap,\n",
      "            oob_score=oob_score,\n",
      "            n_jobs=n_jobs,\n",
      "            random_state=random_state,\n",
      "            verbose=verbose)\n",
      "\n",
      "        self.criterion = criterion\n",
      "        self.max_depth = max_depth\n",
      "        self.min_samples_split = min_samples_split\n",
      "        self.min_samples_leaf = min_samples_leaf\n",
      "        self.max_features = max_features\n",
      "        self.max_leaf_nodes = max_leaf_nodes\n",
      "\n",
      "        if min_density is not None:\n",
      "            warn(\"The min_density parameter is deprecated as of version 0.14 \"\n",
      "                 \"and will be removed in 0.16.\", DeprecationWarning)\n",
      "\n",
      "        if compute_importances is not None:\n",
      "            warn(\"Setting compute_importances is no longer required as \"\n",
      "                 \"version 0.14. Variable importances are now computed on the \"\n",
      "                 \"fly when accessing the feature_importances_ attribute. \"\n",
      "                 \"This parameter will be removed in 0.16.\",\n",
      "                 DeprecationWarning)\n",
      "\n",
      "\n",
      "class RandomForestRegressor(ForestRegressor):\n",
      "    \"\"\"A random forest regressor.\n",
      "\n",
      "    A random forest is a meta estimator that fits a number of classifying\n",
      "    decision trees on various sub-samples of the dataset and use averaging\n",
      "    to improve the predictive accuracy and control over-fitting.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    n_estimators : integer, optional (default=10)\n",
      "        The number of trees in the forest.\n",
      "\n",
      "    criterion : string, optional (default=\"mse\")\n",
      "        The function to measure the quality of a split. The only supported\n",
      "        criterion is \"mse\" for the mean squared error.\n",
      "        Note: this parameter is tree-specific.\n",
      "\n",
      "    max_features : int, float, string or None, optional (default=\"auto\")\n",
      "        The number of features to consider when looking for the best split:\n",
      "          - If int, then consider `max_features` features at each split.\n",
      "          - If float, then `max_features` is a percentage and\n",
      "            `int(max_features * n_features)` features are considered at each\n",
      "            split.\n",
      "          - If \"auto\", then `max_features=n_features`.\n",
      "          - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "          - If \"log2\", then `max_features=log2(n_features)`.\n",
      "          - If None, then `max_features=n_features`.\n",
      "\n",
      "        Note: this parameter is tree-specific.\n",
      "\n",
      "    max_depth : integer or None, optional (default=None)\n",
      "        The maximum depth of the tree. If None, then nodes are expanded until\n",
      "        all leaves are pure or until all leaves contain less than\n",
      "        min_samples_split samples.\n",
      "        Ignored if ``max_samples_leaf`` is not None.\n",
      "        Note: this parameter is tree-specific.\n",
      "\n",
      "    min_samples_split : integer, optional (default=2)\n",
      "        The minimum number of samples required to split an internal node.\n",
      "        Note: this parameter is tree-specific.\n",
      "\n",
      "    min_samples_leaf : integer, optional (default=1)\n",
      "        The minimum number of samples in newly created leaves.  A split is\n",
      "        discarded if after the split, one of the leaves would contain less then\n",
      "        ``min_samples_leaf`` samples.\n",
      "        Note: this parameter is tree-specific.\n",
      "\n",
      "    max_leaf_nodes : int or None, optional (default=None)\n",
      "        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "        Best nodes are defined as relative reduction in impurity.\n",
      "        If None then unlimited number of leaf nodes.\n",
      "        If not None then ``max_depth`` will be ignored.\n",
      "        Note: this parameter is tree-specific.\n",
      "\n",
      "    bootstrap : boolean, optional (default=True)\n",
      "        Whether bootstrap samples are used when building trees.\n",
      "\n",
      "    oob_score : bool\n",
      "        whether to use out-of-bag samples to estimate\n",
      "        the generalization error.\n",
      "\n",
      "    n_jobs : integer, optional (default=1)\n",
      "        The number of jobs to run in parallel for both `fit` and `predict`.\n",
      "        If -1, then the number of jobs is set to the number of cores.\n",
      "\n",
      "    random_state : int, RandomState instance or None, optional (default=None)\n",
      "        If int, random_state is the seed used by the random number generator;\n",
      "        If RandomState instance, random_state is the random number generator;\n",
      "        If None, the random number generator is the RandomState instance used\n",
      "        by `np.random`.\n",
      "\n",
      "    verbose : int, optional (default=0)\n",
      "        Controls the verbosity of the tree building process.\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    `estimators_`: list of DecisionTreeRegressor\n",
      "        The collection of fitted sub-estimators.\n",
      "\n",
      "    `feature_importances_` : array of shape = [n_features]\n",
      "        The feature importances (the higher, the more important the feature).\n",
      "\n",
      "    `oob_score_` : float\n",
      "        Score of the training dataset obtained using an out-of-bag estimate.\n",
      "\n",
      "    `oob_prediction_` : array of shape = [n_samples]\n",
      "        Prediction computed with out-of-bag estimate on the training set.\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "\n",
      "    .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      "\n",
      "    See also\n",
      "    --------\n",
      "    DecisionTreeRegressor, ExtraTreesRegressor\n",
      "    \"\"\"\n",
      "    def __init__(self,\n",
      "                 n_estimators=10,\n",
      "                 criterion=\"mse\",\n",
      "                 max_depth=None,\n",
      "                 min_samples_split=2,\n",
      "                 min_samples_leaf=1,\n",
      "                 max_features=\"auto\",\n",
      "                 max_leaf_nodes=None,\n",
      "                 bootstrap=True,\n",
      "                 oob_score=False,\n",
      "                 n_jobs=1,\n",
      "                 random_state=None,\n",
      "                 verbose=0,\n",
      "                 min_density=None,\n",
      "                 compute_importances=None):\n",
      "        super(RandomForestRegressor, self).__init__(\n",
      "            base_estimator=DecisionTreeRegressor(),\n",
      "            n_estimators=n_estimators,\n",
      "            estimator_params=(\"criterion\", \"max_depth\", \"min_samples_split\",\n",
      "                              \"min_samples_leaf\", \"max_features\",\n",
      "                              \"max_leaf_nodes\", \"random_state\"),\n",
      "            bootstrap=bootstrap,\n",
      "            oob_score=oob_score,\n",
      "            n_jobs=n_jobs,\n",
      "            random_state=random_state,\n",
      "            verbose=verbose)\n",
      "\n",
      "        self.criterion = criterion\n",
      "        self.max_depth = max_depth\n",
      "        self.min_samples_split = min_samples_split\n",
      "        self.min_samples_leaf = min_samples_leaf\n",
      "        self.max_features = max_features\n",
      "        self.max_leaf_nodes = max_leaf_nodes\n",
      "\n",
      "        if min_density is not None:\n",
      "            warn(\"The min_density parameter is deprecated as of version 0.14 \"\n",
      "                 \"and will be removed in 0.16.\", DeprecationWarning)\n",
      "\n",
      "        if compute_importances is not None:\n",
      "            warn(\"Setting compute_importances is no longer required as \"\n",
      "                 \"version 0.14. Variable importances are now computed on the \"\n",
      "                 \"fly when accessing the feature_importances_ attribute. \"\n",
      "                 \"This parameter will be removed in 0.16.\",\n",
      "                 DeprecationWarning)\n",
      "\n",
      "\n",
      "class ExtraTreesClassifier(ForestClassifier):\n",
      "    \"\"\"An extra-trees classifier.\n",
      "\n",
      "    This class implements a meta estimator that fits a number of\n",
      "    randomized decision trees (a.k.a. extra-trees) on various sub-samples\n",
      "    of the dataset and use averaging to improve the predictive accuracy\n",
      "    and control over-fitting.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    n_estimators : integer, optional (default=10)\n",
      "        The number of trees in the forest.\n",
      "\n",
      "    criterion : string, optional (default=\"gini\")\n",
      "        The function to measure the quality of a split. Supported criteria are\n",
      "        \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
      "        Note: this parameter is tree-specific.\n",
      "\n",
      "    max_features : int, float, string or None, optional (default=\"auto\")\n",
      "        The number of features to consider when looking for the best split:\n",
      "          - If int, then consider `max_features` features at each split.\n",
      "          - If float, then `max_features` is a percentage and\n",
      "            `int(max_features * n_features)` features are considered at each\n",
      "            split.\n",
      "          - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      "          - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "          - If \"log2\", then `max_features=log2(n_features)`.\n",
      "          - If None, then `max_features=n_features`.\n",
      "\n",
      "        Note: this parameter is tree-specific.\n",
      "\n",
      "    max_depth : integer or None, optional (default=None)\n",
      "        The maximum depth of the tree. If None, then nodes are expanded until\n",
      "        all leaves are pure or until all leaves contain less than\n",
      "        min_samples_split samples.\n",
      "        Ignored if ``max_samples_leaf`` is not None.\n",
      "        Note: this parameter is tree-specific.\n",
      "\n",
      "    min_samples_split : integer, optional (default=2)\n",
      "        The minimum number of samples required to split an internal node.\n",
      "        Note: this parameter is tree-specific.\n",
      "\n",
      "    min_samples_leaf : integer, optional (default=1)\n",
      "        The minimum number of samples in newly created leaves.  A split is\n",
      "        discarded if after the split, one of the leaves would contain less then\n",
      "        ``min_samples_leaf`` samples.\n",
      "        Note: this parameter is tree-specific.\n",
      "\n",
      "    max_leaf_nodes : int or None, optional (default=None)\n",
      "        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "        Best nodes are defined as relative reduction in impurity.\n",
      "        If None then unlimited number of leaf nodes.\n",
      "        If not None then ``max_depth`` will be ignored.\n",
      "        Note: this parameter is tree-specific.\n",
      "\n",
      "    bootstrap : boolean, optional (default=False)\n",
      "        Whether bootstrap samples are used when building trees.\n",
      "\n",
      "    oob_score : bool\n",
      "        Whether to use out-of-bag samples to estimate\n",
      "        the generalization error.\n",
      "\n",
      "    n_jobs : integer, optional (default=1)\n",
      "        The number of jobs to run in parallel for both `fit` and `predict`.\n",
      "        If -1, then the number of jobs is set to the number of cores.\n",
      "\n",
      "    random_state : int, RandomState instance or None, optional (default=None)\n",
      "        If int, random_state is the seed used by the random number generator;\n",
      "        If RandomState instance, random_state is the random number generator;\n",
      "        If None, the random number generator is the RandomState instance used\n",
      "        by `np.random`.\n",
      "\n",
      "    verbose : int, optional (default=0)\n",
      "        Controls the verbosity of the tree building process.\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    `estimators_`: list of DecisionTreeClassifier\n",
      "        The collection of fitted sub-estimators.\n",
      "\n",
      "    `classes_`: array of shape = [n_classes] or a list of such arrays\n",
      "        The classes labels (single output problem), or a list of arrays of\n",
      "        class labels (multi-output problem).\n",
      "\n",
      "    `n_classes_`: int or list\n",
      "        The number of classes (single output problem), or a list containing the\n",
      "        number of classes for each output (multi-output problem).\n",
      "\n",
      "    `feature_importances_` : array of shape = [n_features]\n",
      "        The feature importances (the higher, the more important the feature).\n",
      "\n",
      "    `oob_score_` : float\n",
      "        Score of the training dataset obtained using an out-of-bag estimate.\n",
      "\n",
      "    `oob_decision_function_` : array of shape = [n_samples, n_classes]\n",
      "        Decision function computed with out-of-bag estimate on the training\n",
      "        set. If n_estimators is small it might be possible that a data point\n",
      "        was never left out during the bootstrap. In this case,\n",
      "        `oob_decision_function_` might contain NaN.\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "\n",
      "    .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n",
      "           Machine Learning, 63(1), 3-42, 2006.\n",
      "\n",
      "    See also\n",
      "    --------\n",
      "    sklearn.tree.ExtraTreeClassifier : Base classifier for this ensemble.\n",
      "    RandomForestClassifier : Ensemble Classifier based on trees with optimal\n",
      "        splits.\n",
      "    \"\"\"\n",
      "    def __init__(self,\n",
      "                 n_estimators=10,\n",
      "                 criterion=\"gini\",\n",
      "                 max_depth=None,\n",
      "                 min_samples_split=2,\n",
      "                 min_samples_leaf=1,\n",
      "                 max_features=\"auto\",\n",
      "                 max_leaf_nodes=None,\n",
      "                 bootstrap=False,\n",
      "                 oob_score=False,\n",
      "                 n_jobs=1,\n",
      "                 random_state=None,\n",
      "                 verbose=0,\n",
      "                 min_density=None,\n",
      "                 compute_importances=None):\n",
      "        super(ExtraTreesClassifier, self).__init__(\n",
      "            base_estimator=ExtraTreeClassifier(),\n",
      "            n_estimators=n_estimators,\n",
      "            estimator_params=(\"criterion\", \"max_depth\", \"min_samples_split\",\n",
      "                              \"min_samples_leaf\", \"max_features\",\n",
      "                              \"max_leaf_nodes\", \"random_state\"),\n",
      "            bootstrap=bootstrap,\n",
      "            oob_score=oob_score,\n",
      "            n_jobs=n_jobs,\n",
      "            random_state=random_state,\n",
      "            verbose=verbose)\n",
      "\n",
      "        self.criterion = criterion\n",
      "        self.max_depth = max_depth\n",
      "        self.min_samples_split = min_samples_split\n",
      "        self.min_samples_leaf = min_samples_leaf\n",
      "        self.max_features = max_features\n",
      "        self.max_leaf_nodes = max_leaf_nodes\n",
      "\n",
      "        if min_density is not None:\n",
      "            warn(\"The min_density parameter is deprecated as of version 0.14 \"\n",
      "                 \"and will be removed in 0.16.\", DeprecationWarning)\n",
      "\n",
      "        if compute_importances is not None:\n",
      "            warn(\"Setting compute_importances is no longer required as \"\n",
      "                 \"version 0.14. Variable importances are now computed on the \"\n",
      "                 \"fly when accessing the feature_importances_ attribute. \"\n",
      "                 \"This parameter will be removed in 0.16.\",\n",
      "                 DeprecationWarning)\n",
      "\n",
      "\n",
      "class ExtraTreesRegressor(ForestRegressor):\n",
      "    \"\"\"An extra-trees regressor.\n",
      "\n",
      "    This class implements a meta estimator that fits a number of\n",
      "    randomized decision trees (a.k.a. extra-trees) on various sub-samples\n",
      "    of the dataset and use averaging to improve the predictive accuracy\n",
      "    and control over-fitting.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    n_estimators : integer, optional (default=10)\n",
      "        The number of trees in the forest.\n",
      "\n",
      "    criterion : string, optional (default=\"mse\")\n",
      "        The function to measure the quality of a split. The only supported\n",
      "        criterion is \"mse\" for the mean squared error.\n",
      "        Note: this parameter is tree-specific.\n",
      "\n",
      "    max_features : int, float, string or None, optional (default=\"auto\")\n",
      "        The number of features to consider when looking for the best split:\n",
      "          - If int, then consider `max_features` features at each split.\n",
      "          - If float, then `max_features` is a percentage and\n",
      "            `int(max_features * n_features)` features are considered at each\n",
      "            split.\n",
      "          - If \"auto\", then `max_features=n_features`.\n",
      "          - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "          - If \"log2\", then `max_features=log2(n_features)`.\n",
      "          - If None, then `max_features=n_features`.\n",
      "\n",
      "        Note: this parameter is tree-specific.\n",
      "\n",
      "    max_depth : integer or None, optional (default=None)\n",
      "        The maximum depth of the tree. If None, then nodes are expanded until\n",
      "        all leaves are pure or until all leaves contain less than\n",
      "        min_samples_split samples.\n",
      "        Ignored if ``max_samples_leaf`` is not None.\n",
      "        Note: this parameter is tree-specific.\n",
      "\n",
      "    min_samples_split : integer, optional (default=2)\n",
      "        The minimum number of samples required to split an internal node.\n",
      "        Note: this parameter is tree-specific.\n",
      "\n",
      "    min_samples_leaf : integer, optional (default=1)\n",
      "        The minimum number of samples in newly created leaves.  A split is\n",
      "        discarded if after the split, one of the leaves would contain less then\n",
      "        ``min_samples_leaf`` samples.\n",
      "        Note: this parameter is tree-specific.\n",
      "\n",
      "    max_leaf_nodes : int or None, optional (default=None)\n",
      "        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "        Best nodes are defined as relative reduction in impurity.\n",
      "        If None then unlimited number of leaf nodes.\n",
      "        If not None then ``max_depth`` will be ignored.\n",
      "        Note: this parameter is tree-specific.\n",
      "\n",
      "    bootstrap : boolean, optional (default=False)\n",
      "        Whether bootstrap samples are used when building trees.\n",
      "        Note: this parameter is tree-specific.\n",
      "\n",
      "    oob_score : bool\n",
      "        Whether to use out-of-bag samples to estimate\n",
      "        the generalization error.\n",
      "\n",
      "    n_jobs : integer, optional (default=1)\n",
      "        The number of jobs to run in parallel for both `fit` and `predict`.\n",
      "        If -1, then the number of jobs is set to the number of cores.\n",
      "\n",
      "    random_state : int, RandomState instance or None, optional (default=None)\n",
      "        If int, random_state is the seed used by the random number generator;\n",
      "        If RandomState instance, random_state is the random number generator;\n",
      "        If None, the random number generator is the RandomState instance used\n",
      "        by `np.random`.\n",
      "\n",
      "    verbose : int, optional (default=0)\n",
      "        Controls the verbosity of the tree building process.\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    `estimators_`: list of DecisionTreeRegressor\n",
      "        The collection of fitted sub-estimators.\n",
      "\n",
      "    `feature_importances_` : array of shape = [n_features]\n",
      "        The feature importances (the higher, the more important the feature).\n",
      "\n",
      "    `oob_score_` : float\n",
      "        Score of the training dataset obtained using an out-of-bag estimate.\n",
      "\n",
      "    `oob_prediction_` : array of shape = [n_samples]\n",
      "        Prediction computed with out-of-bag estimate on the training set.\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "\n",
      "    .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n",
      "           Machine Learning, 63(1), 3-42, 2006.\n",
      "\n",
      "    See also\n",
      "    --------\n",
      "    sklearn.tree.ExtraTreeRegressor: Base estimator for this ensemble.\n",
      "    RandomForestRegressor: Ensemble regressor using trees with optimal splits.\n",
      "    \"\"\"\n",
      "    def __init__(self,\n",
      "                 n_estimators=10,\n",
      "                 criterion=\"mse\",\n",
      "                 max_depth=None,\n",
      "                 min_samples_split=2,\n",
      "                 min_samples_leaf=1,\n",
      "                 max_features=\"auto\",\n",
      "                 max_leaf_nodes=None,\n",
      "                 bootstrap=False,\n",
      "                 oob_score=False,\n",
      "                 n_jobs=1,\n",
      "                 random_state=None,\n",
      "                 verbose=0,\n",
      "                 min_density=None,\n",
      "                 compute_importances=None):\n",
      "        super(ExtraTreesRegressor, self).__init__(\n",
      "            base_estimator=ExtraTreeRegressor(),\n",
      "            n_estimators=n_estimators,\n",
      "            estimator_params=(\"criterion\", \"max_depth\", \"min_samples_split\",\n",
      "                              \"min_samples_leaf\", \"max_features\",\n",
      "                              \"max_leaf_nodes\", \"random_state\"),\n",
      "            bootstrap=bootstrap,\n",
      "            oob_score=oob_score,\n",
      "            n_jobs=n_jobs,\n",
      "            random_state=random_state,\n",
      "            verbose=verbose)\n",
      "\n",
      "        self.criterion = criterion\n",
      "        self.max_depth = max_depth\n",
      "        self.min_samples_split = min_samples_split\n",
      "        self.min_samples_leaf = min_samples_leaf\n",
      "        self.max_features = max_features\n",
      "        self.max_leaf_nodes = max_leaf_nodes\n",
      "\n",
      "        if min_density is not None:\n",
      "            warn(\"The min_density parameter is deprecated as of version 0.14 \"\n",
      "                 \"and will be removed in 0.16.\", DeprecationWarning)\n",
      "\n",
      "        if compute_importances is not None:\n",
      "            warn(\"Setting compute_importances is no longer required as \"\n",
      "                 \"version 0.14. Variable importances are now computed on the \"\n",
      "                 \"fly when accessing the feature_importances_ attribute. \"\n",
      "                 \"This parameter will be removed in 0.16.\",\n",
      "                 DeprecationWarning)\n",
      "\n",
      "\n",
      "class RandomTreesEmbedding(BaseForest):\n",
      "    \"\"\"An ensemble of totally random trees.\n",
      "\n",
      "    An unsupervised transformation of a dataset to a high-dimensional\n",
      "    sparse representation. A datapoint is coded according to which leaf of\n",
      "    each tree it is sorted into. Using a one-hot encoding of the leaves,\n",
      "    this leads to a binary coding with as many ones as trees in the forest.\n",
      "\n",
      "    The dimensionality of the resulting representation is approximately\n",
      "    ``n_estimators * 2 ** max_depth``.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    n_estimators : int\n",
      "        Number of trees in the forest.\n",
      "\n",
      "    max_depth : int\n",
      "        The maximum depth of each tree. If None, then nodes are expanded until\n",
      "        all leaves are pure or until all leaves contain less than\n",
      "        min_samples_split samples.\n",
      "        Ignored if ``max_samples_leaf`` is not None.\n",
      "\n",
      "    min_samples_split : integer, optional (default=2)\n",
      "        The minimum number of samples required to split an internal node.\n",
      "        Note: this parameter is tree-specific.\n",
      "\n",
      "    min_samples_leaf : integer, optional (default=1)\n",
      "        The minimum number of samples in newly created leaves.  A split is\n",
      "        discarded if after the split, one of the leaves would contain less then\n",
      "        ``min_samples_leaf`` samples.\n",
      "        Note: this parameter is tree-specific.\n",
      "\n",
      "    max_leaf_nodes : int or None, optional (default=None)\n",
      "        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "        Best nodes are defined as relative reduction in impurity.\n",
      "        If None then unlimited number of leaf nodes.\n",
      "        If not None then ``max_depth`` will be ignored.\n",
      "        Note: this parameter is tree-specific.\n",
      "\n",
      "    n_jobs : integer, optional (default=1)\n",
      "        The number of jobs to run in parallel for both `fit` and `predict`.\n",
      "        If -1, then the number of jobs is set to the number of cores.\n",
      "\n",
      "    random_state : int, RandomState instance or None, optional (default=None)\n",
      "        If int, random_state is the seed used by the random number generator;\n",
      "        If RandomState instance, random_state is the random number generator;\n",
      "        If None, the random number generator is the RandomState instance used\n",
      "        by `np.random`.\n",
      "\n",
      "    verbose : int, optional (default=0)\n",
      "        Controls the verbosity of the tree building process.\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    `estimators_`: list of DecisionTreeClassifier\n",
      "        The collection of fitted sub-estimators.\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "    .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n",
      "           Machine Learning, 63(1), 3-42, 2006.\n",
      "    .. [2] Moosmann, F. and Triggs, B. and Jurie, F.  \"Fast discriminative\n",
      "           visual codebooks using randomized clustering forests\"\n",
      "           NIPS 2007\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self,\n",
      "                 n_estimators=10,\n",
      "                 max_depth=5,\n",
      "                 min_samples_split=2,\n",
      "                 min_samples_leaf=1,\n",
      "                 max_leaf_nodes=None,\n",
      "                 n_jobs=1,\n",
      "                 random_state=None,\n",
      "                 verbose=0,\n",
      "                 min_density=None):\n",
      "        super(RandomTreesEmbedding, self).__init__(\n",
      "            base_estimator=ExtraTreeRegressor(),\n",
      "            n_estimators=n_estimators,\n",
      "            estimator_params=(\"criterion\", \"max_depth\", \"min_samples_split\",\n",
      "                              \"min_samples_leaf\", \"max_features\",\n",
      "                              \"max_leaf_nodes\", \"random_state\"),\n",
      "            bootstrap=False,\n",
      "            oob_score=False,\n",
      "            n_jobs=n_jobs,\n",
      "            random_state=random_state,\n",
      "            verbose=verbose)\n",
      "\n",
      "        self.criterion = 'mse'\n",
      "        self.max_depth = max_depth\n",
      "        self.min_samples_split = min_samples_split\n",
      "        self.min_samples_leaf = min_samples_leaf\n",
      "        self.max_features = 1\n",
      "        self.max_leaf_nodes = max_leaf_nodes\n",
      "\n",
      "        if min_density is not None:\n",
      "            warn(\"The min_density parameter is deprecated as of version 0.14 \"\n",
      "                 \"and will be removed in 0.16.\", DeprecationWarning)\n",
      "\n",
      "    def _set_oob_score(*args):\n",
      "        raise NotImplementedError(\"OOB score not supported by tree embedding\")\n",
      "\n",
      "    def fit(self, X, y=None):\n",
      "        \"\"\"Fit estimator.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like, shape=(n_samples, n_features)\n",
      "            Input data used to build forests.\n",
      "        \"\"\"\n",
      "        self.fit_transform(X, y)\n",
      "        return self\n",
      "\n",
      "    def fit_transform(self, X, y=None):\n",
      "        \"\"\"Fit estimator and transform dataset.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like, shape=(n_samples, n_features)\n",
      "            Input data used to build forests.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        X_transformed: sparse matrix, shape=(n_samples, n_out)\n",
      "            Transformed dataset.\n",
      "        \"\"\"\n",
      "        X = safe_asarray(X)\n",
      "        rnd = check_random_state(self.random_state)\n",
      "        y = rnd.uniform(size=X.shape[0])\n",
      "        super(RandomTreesEmbedding, self).fit(X, y)\n",
      "        self.one_hot_encoder_ = OneHotEncoder()\n",
      "        return self.one_hot_encoder_.fit_transform(self.apply(X))\n",
      "\n",
      "    def transform(self, X):\n",
      "        \"\"\"Transform dataset.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like, shape=(n_samples, n_features)\n",
      "            Input data to be transformed.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        X_transformed: sparse matrix, shape=(n_samples, n_out)\n",
      "            Transformed dataset.\n",
      "        \"\"\"\n",
      "        return self.one_hot_encoder_.transform(self.apply(X))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "array([1, 3, 5])"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}