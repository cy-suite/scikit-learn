{
 "metadata": {
  "name": "BaggingBaseIPython"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division\n",
      "\n",
      "import itertools\n",
      "import numbers\n",
      "import numpy as np\n",
      "from warnings import warn\n",
      "from abc import ABCMeta, abstractmethod\n",
      "from inspect import getargspec"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.base import ClassifierMixin, RegressorMixin\n",
      "from sklearn.externals.joblib import Parallel, delayed, cpu_count\n",
      "from sklearn.externals import six\n",
      "from sklearn.externals.six.moves import xrange\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.utils import check_random_state, check_arrays, column_or_1d\n",
      "from sklearn.utils.fixes import bincount, unique\n",
      "from sklearn.utils.random import sample_without_replacement\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "\n",
      "\n",
      "from sklearn.ensemble.base import BaseEnsemble\n",
      "\n",
      "\n",
      "__all__ = [\"BaggingClassifier\", \"BaggingRegressor\", \"BaseBagging\"]\n",
      "\n",
      "MAX_INT = np.iinfo(np.int32).max\n",
      "\n",
      "#Bootstrap features function is not implemented here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class BaseBagging(six.with_metaclass(ABCMeta, BaseEnsemble)):\n",
      "    \"\"\"Base class for Bagging\n",
      "     Warning: This class should not be used directly. Use derived classes instead\"\"\"  \n",
      "    \n",
      "    @abstractmethod\n",
      "    def __init__(self, base_estimator=None, n_estimators=10, max_samples=1.0, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0):\n",
      "        super(BaseBagging,self).__init__(base_estimator=base_estimator,n_estimators = n_estimators)\n",
      "        self.max_samples = max_samples\n",
      "        self.bootstrap = bootstrap\n",
      "        self.oob_score = oob_score\n",
      "        self.n_jobs = n_jobs\n",
      "        self.random_state = random_state\n",
      "        self.verbose = verbose\n",
      "        \n",
      "    \n",
      "    \n",
      "    @abstractmethod\n",
      "    def _set_oob_score(self, X, y):\n",
      "        \"\"\"Calculate out of bag predictions and score.\"\"\"\n",
      "        \n",
      "    def _validate_y(self,y):\n",
      "        return y\n",
      "    \n",
      "    def fit(self, X, y, sample_weight=None):\n",
      "        \"\"\"Builds a ensemble of estimators for Bagging from the training set (X, y).\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape = [n_samples, n_features]\n",
      "            The training input samples.\n",
      "\n",
      "        y : array-like, shape = [n_samples]\n",
      "            The target values (integers that correspond to classes in classification, real numbers for regression).\n",
      "\n",
      "        sample_weight : array-like, shape = [n_samples] or None\n",
      "            Sample weights. If None, then samples are equally weighted.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        self : object\n",
      "            Returns self.\n",
      "        \"\"\"\n",
      "        \n",
      "        random_state = check_random_state(self.random_state)\n",
      "        X, y = check_arrays(X, y) #convert data        \n",
      "        \n",
      "        n_samples, self.n_features_ = X.shape # Remap output\n",
      "        y = self._validate_y(y)\n",
      "        \n",
      "        # Check parameters\n",
      "        if isinstance(self.max_samples, (numbers.Integral, np.integer)):\n",
      "            max_samples = self.max_samples\n",
      "        else:  # \"max_samples\" is a float\n",
      "            max_samples = int(self.max_samples * X.shape[0])\n",
      "\n",
      "        if not (0 < max_samples <= X.shape[0]):\n",
      "            raise ValueError(\"max_samples must be in (0, n_samples]\")\n",
      "\n",
      "        if not self.bootstrap and self.oob_score:\n",
      "            raise ValueError(\"Out of bag estimation only available\"\n",
      "                             \" if bootstrap=True\")\n",
      "        \n",
      "        # Parallel loop\n",
      "        n_jobs, n_estimators, starts = _partition_estimators(self)\n",
      "        seeds = random_state.randint(MAX_INT, size=self.n_estimators)\n",
      "        \n",
      "        all_results = Parallel(n_jobs=n_jobs, verbose=self.verbose)(delayed(_parallel_build_estimators)(n_estimators[i], self, X, y, sample_weight,            seeds[starts[i]:starts[i + 1]],\n",
      "                verbose=self.verbose) for i in range(n_jobs))\n",
      "\n",
      "        # Reduce\n",
      "        self.estimators_ = list(itertools.chain.from_iterable(t[0] for t in all_results))\n",
      "        self.estimators_samples_ = list(itertools.chain.from_iterable(t[1] for t in all_results))\n",
      "        self.estimators_features_ = list(itertools.chain.from_iterable(t[2] for t in all_results))\n",
      "\n",
      "        if self.oob_score:\n",
      "            self._set_oob_score(X, y)\n",
      "\n",
      "        return self\n",
      "        \n",
      "            \n",
      "        \n",
      "  \n",
      "        \n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mygenerator = (x*x for x in range(3))\n",
      "for i in mygenerator:\n",
      "    print i\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0\n",
        "1\n",
        "4\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in mygenerator:\n",
      "    print i"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class BaggingClassifier(BaseBagging, ClassifierMixin):\n",
      "    \"\"\"A Bagged classifier.\n",
      "\n",
      "    A Bagging classifier is an ensemble method that fits base\n",
      "    classifiers each on random subsets of the original dataset and then\n",
      "    aggregate their individual predictions by voting to form a final prediction. \n",
      "    Such a meta-estimator can typically be used as\n",
      "    a way to reduce the variance of a black-box estimator (e.g: Decision\n",
      "    tree, KNN, etc.), by introducing randomization into its construction procedure and\n",
      "    then making an ensemble out of it.\n",
      "\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    base_estimator : object or None, This is Compulsory (default=None)\n",
      "            The base estimator to fit on random subsets of the dataset.\n",
      "            If None, default is set to Decision tree classifier .\n",
      "\n",
      "    n_estimators : int, optional (default=10)\n",
      "        The number of base estimators in the ensemble.\n",
      "\n",
      "    max_samples : int or float, optional (default=1.0)\n",
      "        The number of samples to draw from X to train each base estimator.\n",
      "            - If int, then draw `max_samples` samples.\n",
      "            - If float, then draw `max_samples * X.shape[0]` samples.\n",
      "\n",
      "    bootstrap : boolean, optional (default=False)\n",
      "        Whether samples are drawn with replacement.\n",
      "\n",
      "    oob_score : bool\n",
      "        Whether to use out-of-bag samples to estimate\n",
      "        the generalization error.\n",
      "\n",
      "    n_jobs : int, optional (default=1)\n",
      "        The number of jobs to run in parallel for both `fit` and `predict`.\n",
      "        If -1, then the number of jobs is set to the number of cores.\n",
      "\n",
      "    random_state : int, RandomState instance or None, optional (default=None)\n",
      "        If int, random_state is the seed used by the random number generator;\n",
      "        If RandomState instance, random_state is the random number generator;\n",
      "        If None, the random number generator is the RandomState instance used\n",
      "        by `np.random`.\n",
      "\n",
      "    verbose : int, optional (default=0)\n",
      "        Controls the verbosity of the building process.\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    `base_estimator_`: list of estimators\n",
      "        The base estimator from which the ensemble is grown.\n",
      "\n",
      "    `estimators_`: list of estimators\n",
      "        The collection of fitted base estimators.\n",
      "\n",
      "    `estimators_samples_`: list of arrays\n",
      "        The subset of drawn samples (i.e., the in-bag samples) for each base\n",
      "        estimator.\n",
      "\n",
      "    `estimators_features_`: list of arrays\n",
      "        The subset of drawn features for each base estimator.\n",
      "\n",
      "    `classes_`: array of shape = [n_classes]\n",
      "        The classes labels.\n",
      "\n",
      "    `n_classes_`: int or list\n",
      "        The number of classes.\n",
      "\n",
      "    `oob_score_` : float\n",
      "        Score of the training dataset obtained using an out-of-bag estimate.\n",
      "\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, base_estimator=DecisionTreeClassifier(), n_estimators=10, max_samples=1.0, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0):\n",
      "    \n",
      "\n",
      "        super(BaggingClassifier, self).__init__(base_estimator, n_estimators=n_estimators, max_samples=max_samples, bootstrap=bootstrap, oob_score=oob_score, n_jobs=n_jobs, random_state=random_state, verbose=verbose)\n",
      "        \n",
      "    def _validate_estimator(self):\n",
      "        \"\"\"Check the estimator and set the base_estimator_ attribute.\"\"\"\n",
      "        super(BaggingClassifier, self)._validate_estimator(\n",
      "            default=DecisionTreeClassifier())\n",
      "        \n",
      "    def _validate_y(self, y):\n",
      "        y = column_or_1d(y, warn=True)\n",
      "        self.classes_, y = unique(y, return_inverse=True)\n",
      "        self.n_classes_ = len(self.classes_)\n",
      "\n",
      "        return y\n",
      "        \n",
      "    def _set_oob_score(self, X, y):\n",
      "        n_classes_ = self.n_classes_\n",
      "        classes_ = self.classes_\n",
      "        n_samples = y.shape[0]\n",
      "\n",
      "        predictions = np.zeros((n_samples, n_classes_))\n",
      "\n",
      "        for estimator, samples, features in zip(self.estimators_, self.estimators_samples_, self.estimators_features_):\n",
      "            mask = np.ones(n_samples, dtype=np.bool)\n",
      "            mask[samples] = False\n",
      "\n",
      "            try:\n",
      "                predictions[mask, :] += estimator.predict_proba(\n",
      "                    (X[mask, :])[:, features])\n",
      "\n",
      "            except (AttributeError, NotImplementedError):\n",
      "                p = estimator.predict((X[mask, :])[:, features])\n",
      "                j = 0\n",
      "\n",
      "                for i in range(n_samples):\n",
      "                    if mask[i]:\n",
      "                        predictions[i, p[j]] += 1\n",
      "                        j += 1\n",
      "\n",
      "        if (predictions.sum(axis=1) == 0).any():\n",
      "            warn(\"Some inputs do not have OOB scores. \"\n",
      "                 \"This probably means too few estimators were used \"\n",
      "                 \"to compute any reliable oob estimates.\")\n",
      "\n",
      "        oob_decision_function = (predictions /\n",
      "                                 predictions.sum(axis=1)[:, np.newaxis])\n",
      "        oob_score = accuracy_score(y, classes_.take(np.argmax(predictions,\n",
      "                                                              axis=1)))\n",
      "\n",
      "        self.oob_decision_function_ = oob_decision_function\n",
      "        self.oob_score_ = oob_score\n",
      "        \n",
      "        \n",
      "\n",
      "    \n",
      "    def predict(self, X):\n",
      "        \"\"\"Predict class for X.\n",
      "\n",
      "        The predicted class of an input sample is computed as the class with\n",
      "        the highest mean predicted probability. If base estimators do not\n",
      "        implement a ``predict_proba`` method, then it resorts to voting.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape = [n_samples, n_features]\n",
      "            The input samples.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        y : array of shape = [n_samples]\n",
      "            The predicted classes.\n",
      "        \"\"\"\n",
      "        return self.classes_.take(np.argmax(self.predict_proba(X), axis=1), axis=0)\n",
      "    \n",
      "    \n",
      "    def predict_proba(self, X):\n",
      "        \"\"\"Predict class probabilities for X.\n",
      "\n",
      "        The predicted class probabilities of an input sample is computed as\n",
      "        the mean predicted class probabilities of the base estimators in the\n",
      "        ensemble. If base estimators do not implement a ``predict_proba``\n",
      "        method, then it resorts to voting and the predicted class probabilities\n",
      "        of a an input sample represents the proportion of estimators predicting\n",
      "        each class.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape = [n_samples, n_features]\n",
      "            The input samples.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        p : array of shape = [n_samples, n_classes]\n",
      "            The class probabilities of the input samples. Classes are\n",
      "            ordered by arithmetical order.\n",
      "        \"\"\"\n",
      "        # Check data\n",
      "        X, = check_arrays(X)\n",
      "\n",
      "        if self.n_features_ != X.shape[1]:\n",
      "            raise ValueError(\"Number of features of the model must \"\n",
      "                             \"match the input. Model n_features is {0} and \"\n",
      "                             \"input n_features is {1}.\"\n",
      "                             \"\".format(self.n_features_, X.shape[1]))\n",
      "\n",
      "        # Parallel loop\n",
      "        n_jobs, n_estimators, starts = _partition_estimators(self)\n",
      "\n",
      "        all_proba = Parallel(n_jobs=n_jobs, verbose=self.verbose)(\n",
      "            delayed(_parallel_predict_proba)(\n",
      "                self.estimators_[starts[i]:starts[i + 1]],\n",
      "                self.estimators_features_[starts[i]:starts[i + 1]],\n",
      "                X,\n",
      "                self.n_classes_)\n",
      "            for i in range(n_jobs))\n",
      "\n",
      "        # Reduce\n",
      "        proba = sum(all_proba) / self.n_estimators\n",
      "\n",
      "        return proba\n",
      "    \n",
      "    def predict_log_proba(self, X):\n",
      "        \"\"\"Predict class log-probabilities for X.\n",
      "\n",
      "        The predicted class log-probabilities of an input sample is computed as\n",
      "        the log of the mean predicted class probabilities of the base\n",
      "        estimators in the ensemble.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape = [n_samples, n_features]\n",
      "            The input samples.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        p : array of shape = [n_samples, n_classes]\n",
      "            The class log-probabilities of the input samples. Classes are\n",
      "            ordered by arithmetical order.\n",
      "        \"\"\"\n",
      "        if hasattr(self.base_estimator_, \"predict_log_proba\"):\n",
      "            # Check data\n",
      "            X, = check_arrays(X)\n",
      "\n",
      "            if self.n_features_ != X.shape[1]:\n",
      "                raise ValueError(\"Number of features of the model must \"\n",
      "                                 \"match the input. Model n_features is {0} \"\n",
      "                                 \"and input n_features is {1} \"\n",
      "                                 \"\".format(self.n_features_, X.shape[1]))\n",
      "\n",
      "            # Parallel loop\n",
      "            n_jobs, n_estimators, starts = _partition_estimators(self)\n",
      "\n",
      "            all_log_proba = Parallel(n_jobs=n_jobs, verbose=self.verbose)(\n",
      "                delayed(_parallel_predict_log_proba)(\n",
      "                    self.estimators_[starts[i]:starts[i + 1]],\n",
      "                    self.estimators_features_[starts[i]:starts[i + 1]],\n",
      "                    X,\n",
      "                    self.n_classes_)\n",
      "                for i in range(n_jobs))\n",
      "\n",
      "            # Reduce\n",
      "            log_proba = all_log_proba[0]\n",
      "\n",
      "            for j in range(1, len(all_log_proba)):\n",
      "                log_proba = np.logaddexp(log_proba, all_log_proba[j])\n",
      "\n",
      "            log_proba -= np.log(self.n_estimators)\n",
      "\n",
      "            return log_proba\n",
      "\n",
      "        else:\n",
      "            return np.log(self.predict_proba(X))\n",
      "        \n",
      "        \n",
      "    def decision_function(self, X):\n",
      "        \"\"\"Average of the decision functions of the base classifiers.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape = [n_samples, n_features]\n",
      "            The input samples.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        score : array, shape = [n_samples, k]\n",
      "            The decision function of the input samples. The columns correspond\n",
      "            to the classes in sorted order, as they appear in the attribute\n",
      "            ``classes_``. Regression and binary classification are special\n",
      "            cases with ``k == 1``, otherwise ``k==n_classes``.\n",
      "\n",
      "        \"\"\"\n",
      "        # Trigger an exception if not supported\n",
      "        if not hasattr(self.base_estimator, \"decision_function\"):\n",
      "            raise NotImplementedError\n",
      "\n",
      "        # Check data\n",
      "        X, = check_arrays(X)\n",
      "\n",
      "        if self.n_features_ != X.shape[1]:\n",
      "            raise ValueError(\"Number of features of the model must \"\n",
      "                             \"match the input. Model n_features is {1} and \"\n",
      "                             \"input n_features is {2} \"\n",
      "                             \"\".format(self.n_features_, X.shape[1]))\n",
      "\n",
      "        # Parallel loop\n",
      "        n_jobs, n_estimators, starts = _partition_estimators(self)\n",
      "\n",
      "        all_decisions = Parallel(n_jobs=n_jobs, verbose=self.verbose)(\n",
      "            delayed(_parallel_decision_function)(\n",
      "                self.estimators_[starts[i]:starts[i + 1]],\n",
      "                self.estimators_features_[starts[i]:starts[i + 1]],\n",
      "                X)\n",
      "            for i in range(n_jobs))\n",
      "\n",
      "        # Reduce\n",
      "        decisions = sum(all_decisions) / self.n_estimators\n",
      "\n",
      "        return decisions      \n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test = np.ones(5)\n",
      "np.log(test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "array([ 0.,  0.,  0.,  0.,  0.])"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class BaggingRegressor(BaseBagging, RegressorMixin):    \n",
      "    \"\"\" A bagged regressor\n",
      "        \n",
      "        Bagged regressor is an ensemble method which applys same estimator on random subsets of data to build\n",
      "        a new estimator. Original data set is sampled randomly and they are subjected into  regression \n",
      "        seperately, then aggregated(by averaging)to form the final estimator. This ensembled estimator can be\n",
      "        used to reduce the variance of other non-linear black box estimators.(eg: Decistion Tree, KNN, ect.)\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        \n",
      "        base_estimator : object or None, This is Compulsory (default=None)\n",
      "            The base estimator to fit on random subsets of the dataset.\n",
      "            If None, default is set to Decision tree Regressor .\n",
      "\n",
      "        n_estimators : int, optional (default=10)\n",
      "            The number of base estimators in the ensemble.\n",
      "    \n",
      "        max_samples : int or float, optional (default=1.0)\n",
      "            The number of samples to draw from X to train each base estimator.\n",
      "                - If int, then draw `max_samples` samples.\n",
      "                - If float, then draw `max_samples * X.shape[0]` samples.  \n",
      "    \n",
      "        bootstrap : boolean, optional (default=False)\n",
      "            Whether samples are drawn with replacement.\n",
      "    \n",
      "        oob_score : bool\n",
      "            Whether to use out-of-bag samples to estimate\n",
      "            the generalization error.\n",
      "    \n",
      "        n_jobs : int, optional (default=1)\n",
      "            The number of jobs to run in parallel for both `fit` and `predict`.\n",
      "            If -1, then the number of jobs is set to the number of cores.\n",
      "    \n",
      "        random_state : int, RandomState instance or None, optional (default=None)\n",
      "            If int, random_state is the seed used by the random number generator;\n",
      "            If RandomState instance, random_state is the random number generator;\n",
      "            If None, the random number generator is the RandomState instance used\n",
      "            by `np.random`.\n",
      "    \n",
      "        verbose : int, optional (default=0)\n",
      "            Controls the verbosity of the building process.\n",
      "    \n",
      "        Attributes\n",
      "        ----------\n",
      "        `estimators_`: list of estimators\n",
      "            The collection of fitted sub-estimators.\n",
      "    \n",
      "        `estimators_samples_`: list of arrays\n",
      "            The subset of drawn samples (i.e., the in-bag samples) for each base\n",
      "            estimator.\n",
      "    \n",
      "        `estimators_features_`: list of arrays\n",
      "            The subset of drawn features for each base estimator.\n",
      "    \n",
      "        `oob_score_` : float\n",
      "            Score of the training dataset obtained using an out-of-bag estimate.\n",
      "    \n",
      "        `oob_decision_function_` : array of shape = [n_samples, n_classes]\n",
      "            Decision function computed with out-of-bag estimate on the training\n",
      "            set. If n_estimators is small it might be possible that a data point\n",
      "            was never left out during the bootstrap. In this case,\n",
      "            `oob_decision_function_` might contain NaN. \"\"\"\n",
      "    \n",
      "    \n",
      "    def __init__(self, base_estimator=DecisionTreeRegressor(), n_estimators=10, max_samples=1.0, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0): \n",
      "\n",
      "            \n",
      "        super(BaggingRegressor, self).__init__(base_estimator, n_estimators=n_estimators, max_samples=max_samples, bootstrap=bootstrap, oob_score=oob_score, n_jobs=n_jobs, random_state=random_state, verbose=verbose)\n",
      "        \n",
      "    def _validate_estimator(self):\n",
      "        \"\"\"Check the estimator and set the base_estimator_ attribute.\"\"\"\n",
      "        super(BaggingRegressor, self)._validate_estimator(\n",
      "            default=DecisionTreeRegressor())\n",
      "        \n",
      "        \n",
      "   \n",
      "           \n",
      "\n",
      "    \n",
      "    def predict(self, X):\n",
      "        \"\"\"Predicts regression target for X.\n",
      "\n",
      "        The predicted regression target of an input sample is computed as the\n",
      "        average predicted regression targets of the estimators in the ensemble.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape = [n_samples, n_features]\n",
      "            The input samples.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        y: array of shape = [n_samples]\n",
      "            The predicted values.\n",
      "        \"\"\"\n",
      "        # Check data\n",
      "        X, = check_arrays(X)\n",
      "\n",
      "        # Parallel loop\n",
      "        n_jobs, n_estimators, starts = _partition_estimators(self)\n",
      "\n",
      "        all_y = Parallel(n_jobs=n_jobs, verbose=self.verbose)(delayed(_parallel_predict_regression)(self.estimators_[starts[i]:starts[i + 1]], self.estimators_features_[starts[i]:starts[i + 1]], X) for i in range(n_jobs))\n",
      "\n",
      "        # Reduce\n",
      "        y = sum(all_y) / self.n_estimators\n",
      "\n",
      "        return y\n",
      "    \n",
      "    def _set_oob_score(self, X, y):\n",
      "        n_samples = y.shape[0]\n",
      "\n",
      "        predictions = np.zeros((n_samples,))\n",
      "        n_predictions = np.zeros((n_samples,))\n",
      "        \n",
      "        for estimator, samples, features in zip(self.estimators_,\n",
      "                                                self.estimators_samples_,\n",
      "                                                self.estimators_features_):\n",
      "            mask = np.ones(n_samples, dtype=np.bool)\n",
      "            mask[samples] = False\n",
      "\n",
      "            predictions[mask] += estimator.predict((X[mask, :])[:, features])\n",
      "            n_predictions[mask] += 1\n",
      "\n",
      "        if (n_predictions == 0).any():\n",
      "            warn(\"Some inputs do not have OOB scores. \"\n",
      "                 \"This probably means too few estimators were used \"\n",
      "                 \"to compute any reliable oob estimates.\")\n",
      "            n_predictions[n_predictions == 0] = 1\n",
      "\n",
      "        predictions /= n_predictions\n",
      "\n",
      "        self.oob_prediction_ = predictions\n",
      "        self.oob_score_ = r2_score(y, predictions)\n",
      "        #print self.oob_score_\n",
      "\n",
      "        \n",
      "        \n",
      "    \n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "            \n",
      "        \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" There will be many estimators with different samples for the estimator that user would want to use for bagging. Those estimators have to be distributed among jobs.\"\"\"\n",
      "def _partition_estimators(ensemble):\n",
      "    \"\"\"This private function will partition estimators among jobs\"\"\"\n",
      "    \n",
      "    #Computing the number of jobs required\n",
      "    if ensemble.n_jobs == -1:\n",
      "        n_jobs = min(cpu_count(), ensemble.n_estimators)\n",
      "    else: #minimum of n_jobs and n_estimators(user specified) will be taken as the number of jobs\n",
      "        n_jobs = min(ensemble.n_jobs, ensemble.n_estimators) \n",
      "    \n",
      "    #Partition estimators among jobs\n",
      "    n_estimators = (ensemble.n_estimators//n_jobs) * np.ones(n_jobs, dtype = np.int)\n",
      "    n_estimators[:ensemble.n_estimators%n_jobs] += 1\n",
      "    starts = np.cumsum(n_estimators)\n",
      "    \n",
      "    return n_jobs, n_estimators.tolist(), [0] + starts.tolist()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from math import sqrt\n",
      "r = Parallel(n_jobs=-1, verbose=11)(delayed(sqrt)(i**2) for i in range(10))\n",
      "print r\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=-1)]: Done   1 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
        "[Parallel(n_jobs=-1)]: Done   2 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
        "[Parallel(n_jobs=-1)]: Done   3 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
        "[Parallel(n_jobs=-1)]: Done   4 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
        "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
        "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
        "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
        "[Parallel(n_jobs=-1)]: Done   9 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
        "[Parallel(n_jobs=-1)]: Done   8 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
        "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def _parallel_build_estimators(n_estimators, ensemble, X, y, sample_weight, seeds, verbose):\n",
      "    \"\"\"Private function used to build a batch of estimators within a job.\"\"\"\n",
      "    \n",
      "    # Retrieve settings\n",
      "    n_samples, n_features = X.shape\n",
      "    max_samples = ensemble.max_samples\n",
      "    \n",
      "    if (not isinstance(max_samples, (np.integer, numbers.Integral)) and (0.0 < max_samples <= 1.0)):\n",
      "        max_samples = int(max_samples * n_samples)\n",
      "\n",
      "\n",
      "    bootstrap = ensemble.bootstrap\n",
      "    support_sample_weight = (\"sample_weight\" in getargspec(ensemble.base_estimator.fit)[0])\n",
      "\n",
      "    # Build estimators\n",
      "    estimators = []\n",
      "    estimators_samples = []\n",
      "    estimators_features = []\n",
      "\n",
      "    for i in range(n_estimators):\n",
      "        if verbose > 1:\n",
      "            print(\"building estimator %d of %d\" % (i + 1, n_estimators))\n",
      "\n",
      "        random_state = check_random_state(seeds[i])\n",
      "        seed = check_random_state(random_state.randint(MAX_INT))\n",
      "        estimator = ensemble._make_estimator(append=False)\n",
      "\n",
      "        try:  # Not all estimators accept a random_state\n",
      "            estimator.set_params(random_state=seed)\n",
      "        except ValueError:\n",
      "            pass\n",
      "        \n",
      "        #feature bootstraping is not implemented, therefore all features are drawed\n",
      "        #For scalability purposes(add feature bootstrapping), the functionality can be addad here\n",
      "        features = np.array([i for i in range(n_features)])\n",
      "            \n",
      "\n",
      "        # Draw samples, using sample weights, and then fit\n",
      "        if support_sample_weight:\n",
      "            if sample_weight is None:\n",
      "                curr_sample_weight = np.ones((n_samples,))\n",
      "            else:\n",
      "                curr_sample_weight = sample_weight.copy()\n",
      "\n",
      "            if bootstrap:\n",
      "                indices = random_state.randint(0, n_samples, max_samples)\n",
      "                sample_counts = bincount(indices, minlength=n_samples)\n",
      "                curr_sample_weight *= sample_counts\n",
      "\n",
      "            else:\n",
      "                not_indices = sample_without_replacement(\n",
      "                    n_samples,\n",
      "                    n_samples - max_samples,\n",
      "                    random_state=random_state)\n",
      "\n",
      "                curr_sample_weight[not_indices] = 0\n",
      "\n",
      "            estimator.fit(X[:, features], y, sample_weight=curr_sample_weight)\n",
      "            samples = curr_sample_weight > 0.\n",
      "\n",
      "        # Draw samples, using a mask, and then fit\n",
      "        else:\n",
      "            if bootstrap:\n",
      "                indices = random_state.randint(0, n_samples, max_samples)\n",
      "            else:\n",
      "                indices = sample_without_replacement(n_samples,\n",
      "                                                     max_samples,\n",
      "                                                     random_state=random_state)\n",
      "\n",
      "            sample_counts = bincount(indices, minlength=n_samples)\n",
      "\n",
      "            estimator.fit((X[indices])[:, features], y[indices])\n",
      "            samples = sample_counts > 0.\n",
      "\n",
      "        estimators.append(estimator)\n",
      "        estimators_samples.append(samples)\n",
      "        estimators_features.append(features)\n",
      "\n",
      "    return estimators, estimators_samples, estimators_features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def _parallel_predict_proba(estimators, estimators_features, X, n_classes):\n",
      "    \"\"\"Private function used to compute (proba-)predictions within a job.\"\"\"\n",
      "    \n",
      "    n_samples = X.shape[0]\n",
      "    proba = np.zeros((n_samples, n_classes))\n",
      "\n",
      "    for estimator, features in zip(estimators, estimators_features):\n",
      "        try:\n",
      "            proba_estimator = estimator.predict_proba(X[:, features])\n",
      "\n",
      "            if n_classes == len(estimator.classes_):\n",
      "                proba += proba_estimator\n",
      "\n",
      "            else:\n",
      "                proba[:, estimator.classes_] += \\\n",
      "                    proba_estimator[:, range(len(estimator.classes_))]\n",
      "\n",
      "        except (AttributeError, NotImplementedError):\n",
      "            # Resort to voting\n",
      "            predictions = estimator.predict(X[:, features])\n",
      "\n",
      "            for i in range(n_samples):\n",
      "                proba[i, predictions[i]] += 1\n",
      "\n",
      "    return proba"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clss = ['cl1','cl2','cl3', 'cl4']\n",
      "range(len(clss))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "[0, 1, 2, 3]"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def _parallel_predict_log_proba(estimators, estimators_features, X, n_classes):\n",
      "    \"\"\"Private function used to compute log probabilities within a job.\"\"\"\n",
      "    n_samples = X.shape[0]\n",
      "    log_proba = np.empty((n_samples, n_classes))\n",
      "    log_proba.fill(-np.inf)\n",
      "    all_classes = np.arange(n_classes, dtype=np.int)\n",
      "\n",
      "    for estimator, features in zip(estimators, estimators_features):\n",
      "        log_proba_estimator = estimator.predict_log_proba(X[:, features])\n",
      "\n",
      "        if n_classes == len(estimator.classes_):\n",
      "            log_proba = np.logaddexp(log_proba, log_proba_estimator)\n",
      "\n",
      "        else:\n",
      "            log_proba[:, estimator.classes_] = np.logaddexp(log_proba[:, estimator.classes_], log_proba_estimator[:, range(len(estimator.classes_))])\n",
      "\n",
      "            missing = np.setdiff1d(all_classes, estimator.classes_)\n",
      "            log_proba[:, missing] = np.logaddexp(log_proba[:, missing],\n",
      "                                                 -np.inf)\n",
      "\n",
      "    return log_proba\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lp = np.empty((5, 3))\n",
      "lp.fill(-np.inf)\n",
      "lp"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "array([[-inf, -inf, -inf],\n",
        "       [-inf, -inf, -inf],\n",
        "       [-inf, -inf, -inf],\n",
        "       [-inf, -inf, -inf],\n",
        "       [-inf, -inf, -inf]])"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def _parallel_decision_function(estimators, estimators_features, X):\n",
      "    \"\"\"Private function used to compute decisions within a job.\"\"\"\n",
      "    return sum(estimator.decision_function(X[:, features])\n",
      "               for estimator, features in zip(estimators,\n",
      "                                              estimators_features))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "getargspec(RandomForestClassifier.fit)[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "['self', 'X', 'y', 'sample_weight']"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "random_state = check_random_state(5)\n",
      "random_state.randint(3,10,4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "array([6, 9, 8, 9])"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nsample_without_replacement(10,10, random_state = 2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.array([i for i in range(10)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.amax(np.array([3,2,5,2,6,6,75,343,24]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 26,
       "text": [
        "343"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "samples = -1 > 1\n",
      "samples"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 41,
       "text": [
        "False"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def _parallel_predict_regression(estimators, estimators_features, X):\n",
      "    \"\"\"Private funtion which predicts with in a job in regression\"\"\"\n",
      "    return sum(estimator.predict(X[:, features]) for estimator, features in zip(estimators, estimators_features))\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "from sklearn.utils.testing import assert_array_equal\n",
      "from sklearn.utils.testing import assert_array_almost_equal\n",
      "from sklearn.utils.testing import assert_equal\n",
      "from sklearn.utils.testing import assert_raises\n",
      "from sklearn.utils.testing import assert_greater\n",
      "from sklearn.utils.testing import assert_less\n",
      "from sklearn.utils.testing import assert_true\n",
      "from sklearn.utils.testing import assert_warns\n",
      "from sklearn.utils.testing import SkipTest\n",
      "\n",
      "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
      "from sklearn.grid_search import GridSearchCV, ParameterGrid\n",
      "#from sklearn.ensemble import BaggingClassifier, BaggingRegressor\n",
      "from sklearn.linear_model import Perceptron, LogisticRegression\n",
      "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
      "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.svm import SVC, SVR\n",
      "\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.datasets import load_boston, load_iris, load_diabetes\n",
      "from sklearn.utils import check_random_state\n",
      "\n",
      "rng = check_random_state(0)\n",
      "\n",
      "# also load the iris dataset\n",
      "# and randomly permute it\n",
      "iris = load_iris()\n",
      "perm = rng.permutation(iris.target.size)\n",
      "iris.data = iris.data[perm]\n",
      "iris.target = iris.target[perm]\n",
      "\n",
      "# also load the boston dataset\n",
      "# and randomly permute it\n",
      "boston = load_boston()\n",
      "perm = rng.permutation(boston.target.size)\n",
      "boston.data = boston.data[perm]\n",
      "boston.target = boston.target[perm]\n",
      "\n",
      "def test__partition_estimators():\n",
      "    \"\"\"test for parttition_estimators() function\"\"\"\n",
      "    ensemble = RandomForestClassifier(n_estimators=13, n_jobs= 5)\n",
      "    \n",
      "    assert_equal((5,[3,3,3,2,2],[0,3,6,9,11,13]),_partition_estimators(ensemble))\n",
      "    \n",
      "def test__parallel_build_estimators():\n",
      "    \"\"\"test for _parallel_build_estimators() function\"\"\"\n",
      "    ensemble_1 = BaggingRegressor(base_estimator = RandomForestClassifier())\n",
      "    #ensemble_2 = BaggingRegressor(None)\n",
      "    seeds = np.arange(5)\n",
      "    try:\n",
      "        res = _parallel_build_estimators(5, ensemble_1, iris.data, iris.target, sample_weight = None, seeds = seeds, verbose = True)\n",
      "        assert_true(isinstance(res,tuple))\n",
      "    except ValueError:\n",
      "        print ValueError\n",
      "\n",
      "def test_regression():\n",
      "    \"\"\"Check regression for various parameter settings.\"\"\"\n",
      "    rng = check_random_state(0)\n",
      "    X_train, X_test, y_train, y_test = train_test_split(boston.data,\n",
      "                                                        boston.target,\n",
      "                                                        random_state=rng)\n",
      "    grid = ParameterGrid({\"max_samples\": [0.5, 1.0,100],                          \n",
      "                          \"bootstrap\": [True, False]})\n",
      "\n",
      "    for base_estimator in [DummyRegressor(),\n",
      "                           DecisionTreeRegressor(),\n",
      "                           KNeighborsRegressor(),\n",
      "                           SVR()]:\n",
      "        for params in grid:\n",
      "            BaggingRegressor(base_estimator=base_estimator,\n",
      "                             random_state=rng,\n",
      "                             **params).fit(X_train, y_train).predict(X_test)\n",
      "        \n",
      "def test__set_oob_score_regression():\n",
      "    \"\"\"Evaluate out of bag prediction accuracy for regression for various parameter settings.\"\"\"\n",
      "    rng = check_random_state(0)\n",
      "    X_train, X_test, y_train, y_test = train_test_split(boston.data,\n",
      "                                                        boston.target,\n",
      "                                                        random_state=rng)\n",
      "    grid = ParameterGrid({\"max_samples\": [0.5, 1.0,100,300]                       \n",
      "                          })\n",
      "\n",
      "    for base_estimator in [DummyRegressor(),\n",
      "                           DecisionTreeRegressor(),\n",
      "                           KNeighborsRegressor(),\n",
      "                           SVR()]:\n",
      "        for params in grid:\n",
      "            #print base_estimator, params\n",
      "            \n",
      "            BaggingRegressor(base_estimator=base_estimator,\n",
      "                             random_state=rng,oob_score=True,\n",
      "                             **params).fit(X_train, y_train).predict(X_test)\n",
      "            #print\n",
      "\n",
      "def test_bootstrap_samples():\n",
      "    \"\"\"Test that bootstraping samples generate non-perfect base estimators.\"\"\"\n",
      "    rng = check_random_state(0)\n",
      "    X_train, X_test, y_train, y_test = train_test_split(boston.data,\n",
      "                                                        boston.target,\n",
      "                                                        random_state=rng)\n",
      "\n",
      "    base_estimator = DecisionTreeRegressor().fit(X_train, y_train)\n",
      "\n",
      "    # without bootstrap, all trees are perfect on the training set\n",
      "    ensemble = BaggingRegressor(base_estimator=DecisionTreeRegressor(),\n",
      "                                max_samples=1.0,\n",
      "                                bootstrap=False,\n",
      "                                random_state=rng).fit(X_train, y_train)\n",
      "\n",
      "    assert_equal(base_estimator.score(X_train, y_train),\n",
      "                 ensemble.score(X_train, y_train))\n",
      "\n",
      "    # with bootstrap, trees are no longer perfect on the training set\n",
      "    ensemble = BaggingRegressor(base_estimator=DecisionTreeRegressor(),\n",
      "                                max_samples=1.0,\n",
      "                                bootstrap=True,\n",
      "                                random_state=rng).fit(X_train, y_train)\n",
      "\n",
      "    assert_greater(base_estimator.score(X_train, y_train),\n",
      "                   ensemble.score(X_train, y_train))\n",
      "\n",
      "\n",
      "def test_oob_score_regression():\n",
      "    \"\"\"Check that oob prediction is a good estimation of the generalization error.\"\"\"\n",
      "    rng = check_random_state(0)\n",
      "    X_train, X_test, y_train, y_test = train_test_split(boston.data,\n",
      "                                                        boston.target,\n",
      "                                                        random_state=rng)\n",
      "\n",
      "    clf = BaggingRegressor(base_estimator=DecisionTreeRegressor(),\n",
      "                           n_estimators=50,\n",
      "                           bootstrap=True,\n",
      "                           oob_score=True,\n",
      "                           random_state=rng).fit(X_train, y_train)\n",
      "\n",
      "    test_score = clf.score(X_test, y_test)\n",
      "\n",
      "    assert_less(abs(test_score - clf.oob_score_), 0.1)\n",
      "\n",
      "    \n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test__partition_estimators()\n",
      "test__parallel_build_estimators()\n",
      "test_regression()\n",
      "test__set_oob_score_regression()\n",
      "test_bootstrap_samples()\n",
      "test_oob_score_regression()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "-c:126: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grid = ParameterGrid({\"max_samples\": [0.5, 1.0,15],               \n",
      "                          \"bootstrap\": [True, False]})\n",
      "\n",
      "for params in grid:\n",
      "    print params\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'max_samples': 0.5, 'bootstrap': True}\n",
        "{'max_samples': 1.0, 'bootstrap': True}\n",
        "{'max_samples': 15, 'bootstrap': True}\n",
        "{'max_samples': 0.5, 'bootstrap': False}\n",
        "{'max_samples': 1.0, 'bootstrap': False}\n",
        "{'max_samples': 15, 'bootstrap': False}\n"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(<class '__main__.C'>, <class '__main__.A'>, <class '__main__.B'>, <class '__main__.T'>, <type 'object'>)\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "p = np.ones(6,dtype = int)\n",
      "print p"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1 1 1 1 1 1]\n"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "c = np.cumsum(p)\n",
      "print c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1 2 3 4 5 6]\n"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = 2*np.ones(5,dtype=np.int)\n",
      "print s"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[2 2 2 2 2]\n"
       ]
      }
     ],
     "prompt_number": 73
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s[:2]+=1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 74
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 75,
       "text": [
        "array([3, 3, 2, 2, 2])"
       ]
      }
     ],
     "prompt_number": 75
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t = np.cumsum(s)\n",
      "t"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 62,
       "text": [
        "array([ 3,  6,  8, 10, 12])"
       ]
      }
     ],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[0] + t.tolist()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 64,
       "text": [
        "[0, 3, 6, 8, 10, 12]"
       ]
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Compare performance"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import cross_validation\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "\n",
      "iris = load_iris()\n",
      "print 'iris_shape: ' + str(np.shape(iris.data))\n",
      "boston = load_boston()\n",
      "print 'boston_shape: ' + str(np.shape(boston.data))\n",
      "diabetes = load_diabetes()\n",
      "print 'diabetes_shape: ' + str(np.shape(diabetes.data))\n",
      "\n",
      "\n",
      "for base_estimator in [DummyRegressor(),\n",
      "                           DecisionTreeRegressor(),\n",
      "                           KNeighborsRegressor(),\n",
      "                           SVR(),\n",
      "                           LinearRegression(),\n",
      "                           RandomForestRegressor()]:\n",
      "    print base_estimator\n",
      "    print cross_validation.cross_val_score(base_estimator, diabetes.data, diabetes.target)\n",
      "    print cross_validation.cross_val_score(BaggingRegressor(base_estimator=base_estimator, random_state=0, max_samples=0.6, n_jobs=-1), diabetes.data, diabetes.target)\n",
      "    print \n",
      "    \n",
      "    \n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "iris_shape: (150, 4)\n",
        "boston_shape: (506, 13)\n",
        "diabetes_shape: (442, 10)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "DummyRegressor()\n",
        "[ -6.60057638e-03  -1.41317849e-05  -6.03728383e-03]\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ -9.39433205e-03  -2.18482224e-05  -6.77878633e-03]\n",
        "\n",
        "DecisionTreeRegressor(compute_importances=None, criterion=mse, max_depth=None,\n",
        "           max_features=None, min_density=None, min_samples_leaf=1,\n",
        "           min_samples_split=2, random_state=None, splitter=best)\n",
        "[-0.03525502  0.06990687 -0.11396354]\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.44731006  0.43011943  0.33945617]\n",
        "\n",
        "KNeighborsRegressor(algorithm=auto, leaf_size=30, metric=minkowski,\n",
        "          n_neighbors=5, p=2, weights=uniform)\n",
        "[ 0.37363395  0.38558586  0.40697655]\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.38151366  0.3936365   0.50507302]\n",
        "\n",
        "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.0,\n",
        "  kernel=rbf, max_iter=-1, probability=False, random_state=None,\n",
        "  shrinking=True, tol=0.001, verbose=False)\n",
        "[-0.00283284 -0.01851971 -0.05447453]\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.00038213 -0.01797423 -0.07766033]\n",
        "\n",
        "LinearRegression(copy_X=True, fit_intercept=True, normalize=False)\n",
        "[ 0.46930578  0.48724994  0.50955259]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.45426734  0.48699993  0.50882973]\n",
        "\n",
        "RandomForestRegressor(bootstrap=True, compute_importances=None, criterion=mse,\n",
        "           max_depth=None, max_features=auto, min_density=None,\n",
        "           min_samples_leaf=1, min_samples_split=2, n_estimators=10,\n",
        "           n_jobs=1, oob_score=False, random_state=None, verbose=0)\n",
        "[ 0.40172353  0.476825    0.33740723]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.45079646  0.49690844  0.44146134]\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Time   "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import timeit\n",
      "from sklearn.datasets import load_boston\n",
      "boston = load_boston()\n",
      "perm = rng.permutation(boston.target.size)\n",
      "boston.data = boston.data[perm]\n",
      "boston.target = boston.target[perm]\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(boston.data,\n",
      "                                                        boston.target,\n",
      "                                                        random_state=rng)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#%timeit for base_estimator in [DummyRegressor(), DecisionTreeRegressor(), KNeighborsRegressor(), KNeighborsRegressor(), LinearRegression(), RandomForestRegressor()]: BaggingRegressor(base_estimator = base_estimator, random_state=0, max_samples=0.6, n_jobs=-1).fit(X_train, y_train).predict(X_test)                                                                      \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit BaggingRegressor(base_estimator = DummyRegressor(), random_state=0, max_samples=0.6, n_jobs=-1).fit(X_train, y_train).predict(X_test) \n",
      "%timeit BaggingRegressor(base_estimator = DecisionTreeRegressor(), random_state=0, max_samples=0.6, n_jobs=-1).fit(X_train, y_train).predict(X_test)                                                                      \n",
      "%timeit BaggingRegressor(base_estimator = KNeighborsRegressor(), random_state=0, max_samples=0.6, n_jobs=-1).fit(X_train, y_train).predict(X_test)                                                                      \n",
      "%timeit BaggingRegressor(base_estimator = LinearRegression(), random_state=0, max_samples=0.6, n_jobs=-1).fit(X_train, y_train).predict(X_test)                                                                      \n",
      "%timeit BaggingRegressor(base_estimator = RandomForestRegressor(), random_state=0, max_samples=0.6, n_jobs=-1).fit(X_train, y_train).predict(X_test)                                                                      \n",
      "print \n",
      "\n",
      "%timeit DummyRegressor().fit(X_train, y_train).predict(X_test)\n",
      "%timeit DecisionTreeRegressor().fit(X_train, y_train).predict(X_test)\n",
      "%timeit KNeighborsRegressor().fit(X_train, y_train).predict(X_test)\n",
      "%timeit LinearRegression().fit(X_train, y_train).predict(X_test)\n",
      "%timeit RandomForestRegressor().fit(X_train, y_train).predict(X_test)\n",
      "\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 loops, best of 3: 232 ms per loop\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 loops, best of 3: 232 ms per loop\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 loops, best of 3: 236 ms per loop\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 loops, best of 3: 232 ms per loop\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 loops, best of 3: 443 ms per loop\n",
        "\n",
        "10000 loops, best of 3: 72.4 us per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "100 loops, best of 3: 7.35 ms per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1000 loops, best of 3: 1.26 ms per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1000 loops, best of 3: 504 us per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "10 loops, best of 3: 41.6 ms per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}