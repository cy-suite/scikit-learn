"""
The :mod:`sklearn.slda` module implements Shrinkage Linear Discriminant Analysis (sLDA).
"""
from __future__ import print_function
# Authors: Martin Billinger
#          Clemens Brunner

import warnings  # TODO: replace print() with warnings?

import numpy as np

from .base import BaseEstimator, ClassifierMixin
from .utils.extmath import logsumexp
from .utils import check_arrays, array2d, column_or_1d
from .covariance import empirical_covariance, ledoit_wolf


__all__ = ['LDA']


class LDA(BaseEstimator, ClassifierMixin):
    """
    Shrinkage Linear Discriminant Analysis (sLDA)

    A classifier with a linear decision boundary, generated by fitting class
    conditional densities to the data and using Bayes' rule.

    The model fits a Gaussian density to each class, assuming that all classes
    share the same covariance matrix. By default, the covariance matrix is
    estimated with :func:`ledoit_wolf`.

    Parameters
    ----------
    `priors` : array, optional, shape = [n_classes]
        Priors on classes

    `shrinkage` : str, optional
        Shrinkage method, set to 'auto' for Ledoit-Wolf shrinkage
        or None for empirical covariance estimation (no shrinkage).

    Attributes
    ----------
    `coef_` : array-like, shape = [n_classes, n_features]
        Coefficients of the features in the linear decision function.

    `intercept_` : array, shape = [n_classes]
        Intercept (bias) added to the decision function.

    `covariance_` : array-like, shape = [n_features, n_features]
        Covariance matrix (shared by all classes).

    `means_` : array-like, shape = [n_classes, n_features]
        Class means.

    `priors_` : array-like, shape = [n_classes]
        Class priors (sum to 1).

    `xbar_` : float, shape = [n_features]
        Weighted average of class means (weighted with priors).

    Examples
    --------
    >>> import numpy as np
    >>> from sklearn.slda import LDA
    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
    >>> y = np.array([1, 1, 1, 2, 2, 2])
    >>> clf = LDA()
    >>> clf.fit(X, y)
    LDA(priors=None, shrinkage='auto')
    >>> print(clf.predict([[-0.8, -1]]))
    [1]

    """

    def __init__(self, priors=None, shrinkage='auto'):
        self.priors = np.asarray(priors) if priors is not None else None
        self.shrinkage = shrinkage

        # TODO: support equal priors (with priors=='equal')
        if self.priors is not None:
            if (self.priors < 0).any():
                raise ValueError('priors must be non-negative')
            if self.priors.sum() != 1:
                print('warning: the priors do not sum to 1. Renormalizing')
                self.priors = self.priors / self.priors.sum()

        if shrinkage is not None:
            if shrinkage == 'auto':
                self._cov_estimator = lambda x: ledoit_wolf(x)[0]
            else:
                print('warning: unknown shrinkage method, using no shrinkage')
                self._cov_estimator = empirical_covariance
        else:
            self._cov_estimator = empirical_covariance

    def fit(self, X, y):
        """
        Fit the LDA model according to the given training data and parameters.

        Parameters
        ----------
        X : array-like, shape = [n_samples, n_features]
            Training vector, where n_samples in the number of samples and
            n_features is the number of features.

        y : array, shape = [n_samples]
            Target values (integers).

        """
        X, y = check_arrays(X, y, sparse_format='dense')
        X = array2d(X.T).T  # make sure samples are in columns if X is 1D
        y = column_or_1d(y, warn=True)
        self.classes_, y = np.unique(y, return_inverse=True)
        n_samples, n_features = X.shape
        n_classes = len(self.classes_)
        if n_classes < 2:
            raise ValueError('y has less than 2 classes')
        if self.priors is None:
            self.priors_ = np.bincount(y) / float(n_samples)
        else:
            self.priors_ = self.priors

        means = []
        covs = []
        Xc = []
        for ind in range(n_classes):
            Xg = X[y == ind, :]
            meang = Xg.mean(0)
            means.append(meang)
            Xgc = Xg - meang
            Xc.append(Xgc)
            covg = self._cov_estimator(Xgc)
            covg = np.atleast_2d(covg)
            covs.append(covg)

        self.means_ = np.asarray(means)
        self.xbar_ = np.dot(self.priors_, self.means_)

        Sw = np.mean(covs, 0)  # Within-class scatter  # TODO: weight covariances with priors?
        means = self.means_ - self.xbar_

        self.coef_ = np.linalg.solve(Sw, means.T).T
        self.intercept_ = -0.5 * np.diag(np.dot(means, self.coef_.T)) + np.log(self.priors_)

        return self

    def _decision_function(self, X):
        X = array2d(np.asarray(X).T).T  # make sure samples are in columns if X is 1D
        return np.dot(X - self.xbar_, self.coef_.T) + self.intercept_

    def decision_function(self, X):
        """
        This function returns the decision function values related to each
        class on an array of test vectors X.

        Parameters
        ----------
        X : array-like, shape = [n_samples, n_features]

        Returns
        -------
        C : array, shape = [n_samples, n_classes] or [n_samples,]
            Decision function values related to each class, per sample.
            In the two-class case, the shape is [n_samples,], giving the
            log likelihood ratio of the positive class.
        """
        dec_func = self._decision_function(X)
        if len(self.classes_) == 2:
            return dec_func[:, 1] - dec_func[:, 0]
        return dec_func

    def predict(self, X):
        """
        This function classifies an array of test vectors X. The predicted
        class C for each sample in X is returned.

        Parameters
        ----------
        X : array-like, shape = [n_samples, n_features]

        Returns
        -------
        C : array, shape = [n_samples]
        """
        d = self._decision_function(X)
        y_pred = self.classes_.take(d.argmax(1))
        return y_pred

    def predict_proba(self, X):
        """
        This function returns posterior probabilities of classification
        according to each class on an array of test vectors X.

        Parameters
        ----------
        X : array-like, shape = [n_samples, n_features]

        Returns
        -------
        C : array, shape = [n_samples, n_classes]
        """
        values = self._decision_function(X)
        # compute the likelihood of the underlying gaussian models
        # up to a multiplicative constant.
        likelihood = np.exp(values - values.max(axis=1)[:, np.newaxis])
        # compute posterior probabilities
        return likelihood / likelihood.sum(axis=1)[:, np.newaxis]

    def predict_log_proba(self, X):
        """
        This function returns posterior log-probabilities of classification
        according to each class on an array of test vectors X.

        Parameters
        ----------
        X : array-like, shape = [n_samples, n_features]

        Returns
        -------
        C : array, shape = [n_samples, n_classes]
        """
        values = self._decision_function(X)
        loglikelihood = (values - values.max(axis=1)[:, np.newaxis])
        normalization = logsumexp(loglikelihood, axis=1)
        return loglikelihood - normalization[:, np.newaxis]
